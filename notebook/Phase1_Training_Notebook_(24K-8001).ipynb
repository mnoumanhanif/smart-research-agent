{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Requirements: install needed libraries (run once)\n",
        "%pip install -q datasets transformers ipywidgets tokenizers huggingface_hub tf-keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 12000\n",
            "Val size:   1500\n",
            "Test size:  1500\n",
            "\n",
            "=== TRAIN examples ===\n",
            "\n",
            "--- TRAIN example idx 10476 ---\n",
            "INPUT_TEXT:\n",
            " the in - spiral and coalescence of binary neutron star systems is a topic of increasingly intensive research in observational and theoretical astrophysics. \n",
            " it is anticipated that the first direct detections of gravitational wave ( gw ) will be from compact binary mergers. \n",
            " binary neutron star ( bns ) mergers are also thought to produce short - hard gamma - ray bursts ( sgrb s ) @xcite. \n",
            " simultaneous detections of a prompt gravitational wave signal with a spatially coincident electromagnetic ( em ) counterpart dramatically increases the potential science return of the discovery. \n",
            " for this reason, there has been considerable interest as to which, if any, detectable em signature may result from the merger @xcite. \n",
            " other than sgrbs and their afterglows, including those viewed off - axis @xcite, suggestions include optical afterglows associated with the radio - active decay of tidally expelled r - process material@xcite ( though detailed calculations indicate they are faint @xcite ),  \n",
            "\n",
            "TARGET_TEXT:\n",
            " the simultaneous detection of electromagnetic and gravitational wave emission from merging neutron star binaries would aid greatly in their discovery and interpretation. by studying turbulent amplification of magnetic fields in local high - resolution simulations of neutron star merger conditions, \n",
            " we demonstrate that magnetar - level ( @xmath0 g ) fields are present throughout the merger duration. \n",
            " we find that the small - scale turbulent dynamo converts 60% of the randomized kinetic energy into magnetic fields on a merger time scale. \n",
            " since turbulent magnetic energy dissipates through reconnection events which accelerate relativistic electrons, turbulence may facilitate the conversion of orbital kinetic energy into radiation. \n",
            " if @xmath1 of the @xmath2 erg of orbital kinetic available gets processed through reconnection, and creates radiation in the 15 - 150 kev band, then the fluence at 200 mpc would be @xmath3, potentially rendering most merging neutron stars in the advanced li \n",
            "\n",
            "\n",
            "--- TRAIN example idx 1824 ---\n",
            "INPUT_TEXT:\n",
            " collisions of energetic electrons with highly charged ions ( hci ), abundant in hot plasmas, may lead to emission of anisotropic and polarized characteristic x rays due to an anisotropy of the electron momentum distribution. \n",
            " measurement of polarization of x - ray lines can provide information on the directionality of the electron currents and the orientation of the magnetic field lines in both hot astrophysical and laboratory plasmas @xcite. \n",
            " such diagnostics requires detailed knowledge of polarization properties of atomic processes leading to the x - ray emission. \n",
            " dielectronic recombination ( dr ) @xcite is a resonant process in which a free electron is captured into an ion under the simultaneous excitation of a bound electron and the intermediate excited state is stabilized by emitting an x ray. \n",
            " dr is one of the dominant recombination mechanisms in hot plasmas : its cross section at the resonance energy is often orders of magnitude larger than that of competing recombination p \n",
            "\n",
            "TARGET_TEXT:\n",
            " we report linear polarization measurements of x rays emitted due to dielectronic recombination into highly charged krypton ions. \n",
            " the ions in the he - like through o - like charge states were populated in an electron beam ion trap with the electron beam energy adjusted to recombination resonances in order to produce _ _ k__@xmath0 x rays. \n",
            " the x rays were detected with a newly developed compton polarimeter using a beryllium scattering target and 12 silicon x - ray detector diodes sampling the azimuthal distribution of the scattered x rays. \n",
            " the extracted degrees of linear polarization of several dielectronic recombination transitions agree with results of relativistic distorted wave calculations. \n",
            " we also demonstrate a high sensitivity of the polarization to the breit interaction, which is remarkable for a medium-_z _ element like krypton. \n",
            " the experimental results can be used for polarization diagnostics of hot astrophysical and laboratory fusion plasmas. \n",
            "\n",
            "=== VAL examples ===\n",
            "\n",
            "--- VAL example idx 51 ---\n",
            "INPUT_TEXT:\n",
            " at present, qcd can not describe quantitatively the low - energy nucleon - antinucleon interaction, and various phenomenological approaches have been suggested in order to explain numerous experimental data, see, e.g., refs. \n",
            " @xcite and recent reviews @xcite. \n",
            " however, parameters of the models still can not be extracted with a good accuracy from the experimental data @xcite. \n",
            " very recently, renewed interest in low - energy nucleon - antinucleon physics has been stimulated by the experimental observation of a strong enhancement of decay probability at low invariant mass of @xmath3 in the processes @xmath4 @xcite, @xmath5 and @xmath6 @xcite, @xmath7 and @xmath8 @xcite, @xmath9 @xcite. \n",
            " one of the most natural explanation of this enhancement is final state interaction of the proton and antiproton @xcite. \n",
            " a similar phenomenon was observed in the investigation of the proton ( antiproton ) electric, @xmath10, and magnetic, @xmath11, form factors in the process @xmath12 @xcite. \n",
            " namely \n",
            "\n",
            "TARGET_TEXT:\n",
            " we use the paris nucleon - antinucleon optical potential for explanation of experimental data in the process @xmath0 near threshold. \n",
            " it is shown that the cross section and the electromagnetic form factors are very sensitive to the parameters of the potential. \n",
            " it turns out that final - state interaction due to slightly modified absorptive part of the potential allows us to reproduce available experimental data. \n",
            " we also demonstrated that the cross section in @xmath1 channel is larger than that in @xmath2 one, and their ratio is almost energy independent up to 2.2 gev. \n",
            " electromagnetic form factors of proton and neutron. \n",
            " 13.75.cs, 13.66.bc, 13.40.gp \n",
            "\n",
            "\n",
            "--- VAL example idx 563 ---\n",
            "INPUT_TEXT:\n",
            " the discovery of the two strange - charmed mesons @xmath0 and @xmath4 with spin - parity @xmath5 and @xmath6 respectively has triggered hot debate on their nature and under - structures @xcite. \n",
            " there have been a lot of explanations for their nature, for example, conventional @xmath7 states @xcite, two - meson molecular states @xcite, @xmath8 mixing states @xcite and four - quark states @xcite, etc. the mass of the @xmath0 is significantly lower than the values of the @xmath5 state mass from the quark models and lattice simulations @xcite. \n",
            " those two states @xmath0 and @xmath4 lie just below the @xmath9 and @xmath10 threshold respectively which are analogous to the situation that the scalar mesons @xmath11 and @xmath12 lie just below the @xmath13 threshold and couple strongly to the nearby channels. \n",
            " if we take the scalar mesons @xmath11 and @xmath12 as four - quark states with the constituents of scalar diquark - antidiquark sub - structures, the masses of the scalar nonet mesons b \n",
            "\n",
            "TARGET_TEXT:\n",
            " in this article, we take the point of view that the charmed scalar meson @xmath0 be a tetraquark state and devote to calculate its mass within the framework of the qcd sum rules approach in the heavy quark limit. \n",
            " the numerical values for the mass of the @xmath0 are consistent with the experimental data, there must be some tetraquark component in the scalar meson @xmath0. \n",
            " detailed discussions about the threshold parameter and borel parameter for the multiquark states are also presented. + zhi - gang wang@xmath1 and shao - long wan@xmath2 + @xmath1 department of physics, north china electric power university, baoding 071003, p. r. china + @xmath2 department of modern physics, university of science and technology of china, hefei 230026, p. r. china + pacs number : 12.38.aw, 12.38.qk key words : @xmath3, qcd sum rules \n",
            "\n",
            "=== TEST examples ===\n",
            "\n",
            "--- TEST example idx 501 ---\n",
            "INPUT_TEXT:\n",
            " that local intergalactic space could harbor substantial amounts of hot, highly ionized gas, either as an extended halo surrounding the galaxy, or as an extended medium pervading the local group, was first suggested by spitzer ( 1956 ) and by kahn and woltjer ( 1959 ). \n",
            " such a medium would have a characteristic temperature of order the virial temperature, estimated to be on the order of @xmath8, and be of very low density, @xmath9 @xmath10 ( @xcite ). \n",
            " its emission, mainly soft thermal line emission from highly ionized metals and weak thermal bremsstrahlung, would be very faint and extremely difficult to detect, and searches for diffuse hot gas have relied on detecting its absorption lines in bright background continuum sources. \n",
            " detailed studies of li - like c and o absorption lines in the uv in background quasar spectra have indeed revealed the presence of gas in the galactic halo ( _ e.g. _, @xcite ) and, more recently, beyond ( _ e.g. _, @xcite ). \n",
            " intergalactic x - ray absorpti \n",
            "\n",
            "TARGET_TEXT:\n",
            " recent observations with the dispersive x - ray spectrometers aboard _ \n",
            " chandra _ and _ newton observatory _ have begun to probe the properties of the x - ray intergalactic medium ( igm ) at small redshifts. using large quantities ( @xmath0950 ksec ) of spectroscopic data acquired using the reflection grating spectrometer ( rgs ) aboard _ newton observatory _, we investigated the intervening material toward three low redshift, high galactic latitude active galactic nuclei ( agns ) with nominally featureless spectra : mrk 421, pks 2155@xmath1304 and 3c 273. \n",
            " each spectrum provides clear evidence for what appears to be a local ( @xmath2 ), highly ionized absorbing medium betrayed by the 1s2p resonance transition feature seen at 21.6(@xmath3 ). \n",
            " measurements are also made for the ly @xmath4 transition of the adjacent ionization state, (; \n",
            " 18.97 ), which potentially constrains the absorber s temperature. \n",
            " finally, in a collisional equilibrium approximation, upper limits to diffuse emi \n",
            "\n",
            "\n",
            "--- TEST example idx 457 ---\n",
            "INPUT_TEXT:\n",
            " _ plasmodium falciparum _ malaria is a major parasitic disease which causes severe morbidity and mortality in approximately half a million people annually @xcite. \n",
            " artemisinin ( art ) and its derivatives ( e.g. artesunate, dihydroartemisinin and artemether ), used in combination with partner drugs, provide front - line protection, and have been responsible for dramatic reductions in disease burden over the past few decades @xcite. despite their clinical and public health effectiveness, the emergence of art resistance and lack of alternative treatments places current control programs at risk @xcite. \n",
            " development of a comprehensive understanding of art s mechanism of action and associated effects on infected red blood cells ( irbcs ) is therefore critical for development of optimised art - based treatment regimens and maintenance of control program impact @xcite. \n",
            " recent _ in vitro _ \n",
            " experiments @xcite, combined with advances in pharmacokinetic \n",
            " pharmacodynamic ( pk pd ) modelling  \n",
            "\n",
            "TARGET_TEXT:\n",
            " falciparum malaria is a major parasitic disease causing widespread morbidity and mortality globally. \n",
            " artemisinin derivatives the most effective and widely - used antimalarials that have helped reduce the burden of malaria by 60% in some areas over the past decade have recently been found to induce growth retardation of blood - stage _ plasmodium falciparum _ when applied at clinically relevant concentrations. to date, no model has been designed to quantify the growth retardation effect and to predict the influence of this property on _ in vivo _ parasite killing. here \n",
            " we introduce a mechanistic model of parasite growth from the ring to trophozoite stage of the parasite s life cycle, and by modelling the level of staining with an rna - binding dye, we demonstrate that the model is able to reproduce fluorescence distribution data from _ in vitro _ experiments using the laboratory 3d7 strain. \n",
            " we quantify the dependence of growth retardation on drug concentration and demonstrate the  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data preparation cell for arXiv summarization with truncation & JSONL export\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "seed = 42\n",
        "max_input_length = 4096\n",
        "max_target_length = 512\n",
        "random.seed(seed)\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "\n",
        "# --------------------\n",
        "# Load dataset and select 15,000 random samples\n",
        "# --------------------\n",
        "raw_dataset = load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
        "raw_subset = raw_dataset.shuffle(seed=seed).select(range(15000))\n",
        "\n",
        "# --------------------\n",
        "# Ensure tokenizer (use existing `tokenizer` if already defined)\n",
        "# --------------------\n",
        "try:\n",
        "    tokenizer  # noqa: F821\n",
        "except NameError:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
        "\n",
        "# --------------------\n",
        "# Text cleaning and truncation helpers\n",
        "# --------------------\n",
        "def clean_text(text: str) -> str:\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    # Collapse consecutive spaces/tabs\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    # Collapse repeated blank lines\n",
        "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def truncate_with_tokenizer(text: str, max_length: int) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "    return tokenizer.decode(enc[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Process subset: clean + truncate\n",
        "# --------------------\n",
        "processed_examples = []\n",
        "for ex in raw_subset:\n",
        "    article = clean_text(ex.get(\"article\", \"\"))\n",
        "    abstract = clean_text(ex.get(\"abstract\", \"\"))\n",
        "\n",
        "    input_text = truncate_with_tokenizer(article, max_input_length)\n",
        "    target_text = truncate_with_tokenizer(abstract, max_target_length)\n",
        "\n",
        "    processed_examples.append(\n",
        "        {\n",
        "            \"input_text\": input_text,\n",
        "            \"target_text\": target_text,\n",
        "        }\n",
        "    )\n",
        "\n",
        "dataset = Dataset.from_list(processed_examples)\n",
        "\n",
        "# --------------------\n",
        "# Train/Val/Test split: 80/10/10\n",
        "# --------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tv_split = dataset.train_test_split(test_size=0.2, seed=seed)\n",
        "train_dataset = tv_split[\"train\"]          # 80%\n",
        "temp_dataset = tv_split[\"test\"]           # remaining 20%\n",
        "\n",
        "vt_split = temp_dataset.train_test_split(test_size=0.5, seed=seed)\n",
        "val_dataset = vt_split[\"train\"]           # 10%\n",
        "test_dataset = vt_split[\"test\"]           # 10%\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Save as JSONL\n",
        "# --------------------\n",
        "\n",
        "def save_jsonl(ds: Dataset, path: str):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for ex in ds:\n",
        "            f.write(\n",
        "                json.dumps(\n",
        "                    {\n",
        "                        \"input_text\": ex[\"input_text\"],\n",
        "                        \"target_text\": ex[\"target_text\"],\n",
        "                    },\n",
        "                    ensure_ascii=False,\n",
        "                )\n",
        "                + \"\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "save_jsonl(train_dataset, \"./data/train.jsonl\")\n",
        "save_jsonl(val_dataset, \"./data/val.jsonl\")\n",
        "save_jsonl(test_dataset, \"./data/test.jsonl\")\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Print counts and show 2 random examples from each split\n",
        "# --------------------\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size:   {len(val_dataset)}\")\n",
        "print(f\"Test size:  {len(test_dataset)}\\n\")\n",
        "\n",
        "\n",
        "def show_random_examples(ds: Dataset, name: str, k: int = 2):\n",
        "    print(f\"=== {name} examples ===\")\n",
        "    indices = random.sample(range(len(ds)), k=min(k, len(ds)))\n",
        "    for i in indices:\n",
        "        ex = ds[i]\n",
        "        print(f\"\\n--- {name} example idx {i} ---\")\n",
        "        print(\"INPUT_TEXT:\\n\", ex[\"input_text\"][:1000], \"\\n\")\n",
        "        print(\"TARGET_TEXT:\\n\", ex[\"target_text\"][:1000], \"\\n\")\n",
        "\n",
        "\n",
        "show_random_examples(train_dataset, \"TRAIN\", k=2)\n",
        "show_random_examples(val_dataset, \"VAL\", k=2)\n",
        "show_random_examples(test_dataset, \"TEST\", k=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device count: 1\n"
          ]
        }
      ],
      "source": [
        "   import torch\n",
        "   print(\"CUDA available:\", torch.cuda.is_available())\n",
        "   print(\"Device count:\", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-25 10:19:03.024471: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-25 10:19:03.053575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/workspace/venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "2025-11-25 10:19:03.776542: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce RTX 5090\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6532d95108594d398384533217819f9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c59c75b351e04a50b36459e388d15a35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea5460e71b3f4574b43f1818811a69b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train data:   0%|          | 0/12000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8945ca3f46b43ecbddaa5a3311e8647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing val data:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25ef771666d64a91bcc3df4bf67b7897",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f33fae671c9b4973a087937f6660c79f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bf7d73d560e4c368f92102d4dcbca20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72eee84487e94881b61a242a4c1b1d3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "925513d53d3e4bfca9a0b68ca34c974a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bdc8b32cbeb41908785c3727eb22216",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dda078fb0f2c44dbb3e4a86931e339fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e02eebac97fc423bbef4dc81c6fae595",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "554e615e06194601a4d1daf3a89878f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_351/667070552.py:178: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6000/6000 18:23:13, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.490800</td>\n",
              "      <td>1.445671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.478700</td>\n",
              "      <td>1.429347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.445000</td>\n",
              "      <td>1.421315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.440500</td>\n",
              "      <td>1.416832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.436600</td>\n",
              "      <td>1.413535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.455300</td>\n",
              "      <td>1.410811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.378000</td>\n",
              "      <td>1.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.401600</td>\n",
              "      <td>1.408062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.460800</td>\n",
              "      <td>1.407226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.431700</td>\n",
              "      <td>1.406914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.414200</td>\n",
              "      <td>1.406769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.423400</td>\n",
              "      <td>1.406617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete.\n",
            "LoRA adapter saved to: ./lora_summarizer_llama3/lora_adapter\n",
            "Full model (base) saved to: ./lora_summarizer_llama3/full_model\n"
          ]
        }
      ],
      "source": [
        "# GPU LoRA fine-tuning cell for transformers==4.57.2 (replace your current training cell with this)\n",
        "\n",
        "%pip install -q \"transformers==4.57.2\" \"peft>=0.12.0\" \"accelerate>=0.33.0\" datasets bitsandbytes\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA GPU not available. Check your environment.\"\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3-8B\"\n",
        "DATA_DIR = \"./data\"\n",
        "OUTPUT_DIR = \"./lora_summarizer_llama3\"\n",
        "\n",
        "max_input_length = 4096\n",
        "max_target_length = 512\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------\n",
        "# Tokenizer\n",
        "# ----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# ----------------------\n",
        "# Load JSONL datasets\n",
        "# ----------------------\n",
        "data_files = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"train.jsonl\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"val.jsonl\"),\n",
        "}\n",
        "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "\n",
        "# ----------------------\n",
        "# Tokenization\n",
        "# ----------------------\n",
        "def tokenize_example(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    prompt = (\n",
        "        \"Summarize the following scientific article.\\n\\n\"\n",
        "        \"Article:\\n\"\n",
        "        f\"{example['input_text']}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "    target = example[\"target_text\"]\n",
        "\n",
        "    prompt_ids = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "        add_special_tokens=False,\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    target_ids = tokenizer(\n",
        "        target,\n",
        "        truncation=True,\n",
        "        max_length=max_target_length,\n",
        "        add_special_tokens=False,\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    input_ids = prompt_ids + target_ids + ([eos_id] if eos_id is not None else [])\n",
        "    labels = [-100] * len(prompt_ids) + target_ids + ([eos_id] if eos_id is not None else [])\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_example,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing train data\",\n",
        ")\n",
        "tokenized_eval = eval_dataset.map(\n",
        "    tokenize_example,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        "    desc=\"Tokenizing val data\",\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# Base model on GPU\n",
        "# ----------------------\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# LoRA\n",
        "# ----------------------\n",
        "def get_lora_target_modules(m):\n",
        "    module_names = {name.split(\".\")[-1] for name, _ in m.named_modules()}\n",
        "    if \"q_proj\" in module_names and \"v_proj\" in module_names:\n",
        "        return [\"q_proj\", \"v_proj\"]\n",
        "    if \"q\" in module_names and \"v\" in module_names:\n",
        "        return [\"q\", \"v\"]\n",
        "    return [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "target_modules = get_lora_target_modules(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=target_modules,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# recommended for PEFT + checkpointing\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# ----------------------\n",
        "# Data collator\n",
        "# ----------------------\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=\"longest\",\n",
        "    label_pad_token_id=-100,\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# TrainingArguments (note: eval_strategy, optim from your signature)\n",
        "# ----------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=False,\n",
        "    bf16=True,  # RTX 5090 supports bf16\n",
        "    gradient_checkpointing=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ----------------------\n",
        "# Save adapter + (optional) base model\n",
        "# ----------------------\n",
        "adapter_dir = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
        "full_model_dir = os.path.join(OUTPUT_DIR, \"full_model\")\n",
        "\n",
        "os.makedirs(adapter_dir, exist_ok=True)\n",
        "os.makedirs(full_model_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "\n",
        "base_model = getattr(model, \"base_model\", None)\n",
        "if base_model is not None:\n",
        "    base_model.save_pretrained(full_model_dir)\n",
        "    tokenizer.save_pretrained(full_model_dir)\n",
        "\n",
        "print(\"Training complete.\")\n",
        "print(f\"LoRA adapter saved to: {adapter_dir}\")\n",
        "print(f\"Full model (base) saved to: {full_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace\n",
            "['jupyter', 'lora_summarizer_llama3', 'data', 'venv', 'tensorflow-2.20.0.dev0+selfbuilt-cp312-cp312-linux_x86_64.whl', 'tensorflow']\n",
            "['lora_adapter', 'checkpoint-5500', 'checkpoint-6000', 'full_model', 'runs']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(os.getcwd())  # should be /data/jupyter/Mujtaba/Assignment4\n",
        "print(os.listdir(\".\"))  # see if lora_summarizer_llama3 is here\n",
        "\n",
        "if os.path.isdir(\"lora_summarizer_llama3\"):\n",
        "    print(os.listdir(\"lora_summarizer_llama3\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce RTX 5090\n",
            "Loaded 1500 test samples, using 10 for comparison.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d72241d4f4e34270b2a675bb8055dd3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./lora_summarizer_llama3/full_model were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ./lora_summarizer_llama3/full_model and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28d35bd6ae0b4da0af8c3f3ab217bf58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./lora_summarizer_llama3/full_model were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ./lora_summarizer_llama3/full_model and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Example 0 =====\n",
            "Input (truncated):\n",
            "the current paradigm for the electronic structure of manganites ( a@xmath0b@xmath7mno@xmath2 ) with x @xmath8 1/2 is a lattice of mn sites with d@xmath9 or d@xmath10 orbital occupancy, with the proportion of each decided by the value of x and orbital ordering ( oo ) of the occupied e@xmath11 orbital on d@xmath10 sites. \n",
            " the corresponding double - exchange model hamiltonian has been studied extensively @xcite. \n",
            " manganites with x @xmath8 1/2 usually exhibit a phase transition @xcite which has be...\n",
            "\n",
            "Ground Truth:\n",
            "unrestricted hartree - fock calculations on la@xmath0ca@xmath1mno@xmath2 ( x = 0.5 and x = 0.67 ) in the full magnetic unit cell show that the magnetic ground states of these compounds consist of ferromagnetic molecules or polarons ordered in herring - bone patterns. \n",
            " each polaron consists of either two or three mn ions separated by o@xmath3 ions with a magnetic moment opposed to those of the mn ions. \n",
            " ferromagnetic coupling within the polarons is strong while coupling between them is relative...\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 31.33 GiB of which 716.12 MiB is free. Including non-PyTorch memory, this process has 29.53 GiB memory in use. Of the allocated memory 28.63 GiB is allocated by PyTorch, and 319.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGround Truth:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(ground_truth[:\u001b[32m500\u001b[39m] + (\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ground_truth) > \u001b[32m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m base_summary = \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m lora_summary = generate_summary(lora_model, input_text)\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBase Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mgenerate_summary\u001b[39m\u001b[34m(model, input_text)\u001b[39m\n\u001b[32m    101\u001b[39m inputs = {k: v.to(model.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m generated_ids = outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:]\n\u001b[32m    114\u001b[39m summary = tokenizer.decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:473\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    472\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/accelerate/hooks.py:360\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    353\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    357\u001b[39m         ):\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    370\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    371\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 31.33 GiB of which 716.12 MiB is free. Including non-PyTorch memory, this process has 29.53 GiB memory in use. Of the allocated memory 28.63 GiB is allocated by PyTorch, and 319.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "DATA_DIR = \"./data\"\n",
        "TEST_PATH = os.path.join(DATA_DIR, \"test.jsonl\")\n",
        "\n",
        "BASE_MODEL_DIR = \"./lora_summarizer_llama3/full_model\"\n",
        "LORA_ADAPTER_DIR = \"./lora_summarizer_llama3/lora_adapter\"\n",
        "\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "JSONL_OUT = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "CSV_OUT = os.path.join(OUTPUT_DIR, \"comparisons.csv\")\n",
        "\n",
        "max_input_length = 4096\n",
        "max_new_tokens = 512\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA GPU not available. Check your environment.\"\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ----------------------\n",
        "# Load test set and sample 10 examples\n",
        "# ----------------------\n",
        "test_records = []\n",
        "with open(TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        test_records.append(json.loads(line))\n",
        "\n",
        "assert len(test_records) > 0, \"Test set is empty.\"\n",
        "samples = random.sample(test_records, k=min(10, len(test_records)))\n",
        "\n",
        "print(f\"Loaded {len(test_records)} test samples, using {len(samples)} for comparison.\")\n",
        "\n",
        "# ----------------------\n",
        "# Load tokenizer and models (base + LoRA)\n",
        "# ----------------------\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, use_fast=True)\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"left\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "BASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"  # same base model you trained\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_DIR,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.eval()\n",
        "\n",
        "lora_base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_DIR,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "lora_model = PeftModel.from_pretrained(lora_base, LORA_ADAPTER_DIR)\n",
        "lora_model.eval()\n",
        "\n",
        "# ----------------------\n",
        "# Generation helper\n",
        "# ----------------------\n",
        "def generate_summary(model, input_text: str) -> str:\n",
        "    prompt = (\n",
        "        \"Summarize the following scientific article.\\n\\n\"\n",
        "        \"Article:\\n\"\n",
        "        f\"{input_text}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    summary = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "    return summary\n",
        "\n",
        "# ----------------------\n",
        "# Run generation and collect outputs\n",
        "# ----------------------\n",
        "results = []\n",
        "for idx, ex in enumerate(samples):\n",
        "    input_text = ex[\"input_text\"]\n",
        "    ground_truth = ex[\"target_text\"]\n",
        "\n",
        "    print(f\"\\n===== Example {idx} =====\")\n",
        "    print(\"Input (truncated):\")\n",
        "    print(input_text[:500] + (\"...\" if len(input_text) > 500 else \"\"))\n",
        "    print(\"\\nGround Truth:\")\n",
        "    print(ground_truth[:500] + (\"...\" if len(ground_truth) > 500 else \"\"))\n",
        "\n",
        "    base_summary = generate_summary(base_model, input_text)\n",
        "    lora_summary = generate_summary(lora_model, input_text)\n",
        "\n",
        "    print(\"\\nBase Summary:\")\n",
        "    print(base_summary[:500] + (\"...\" if len(base_summary) > 500 else \"\"))\n",
        "    print(\"\\nLoRA Summary:\")\n",
        "    print(lora_summary[:500] + (\"...\" if len(lora_summary) > 500 else \"\"))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    results.append(\n",
        "        {\n",
        "            \"id\": idx,\n",
        "            \"input_text\": input_text,\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"base_summary\": base_summary,\n",
        "            \"lora_summary\": lora_summary,\n",
        "        }\n",
        "    )\n",
        "\n",
        "# ----------------------\n",
        "# Save to JSONL\n",
        "# ----------------------\n",
        "with open(JSONL_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in results:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# ----------------------\n",
        "# Save to CSV\n",
        "# ----------------------\n",
        "with open(CSV_OUT, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"id\", \"input_text\", \"ground_truth\", \"base_summary\", \"lora_summary\"])\n",
        "    for row in results:\n",
        "        writer.writerow(\n",
        "            [\n",
        "                row[\"id\"],\n",
        "                row[\"input_text\"],\n",
        "                row[\"ground_truth\"],\n",
        "                row[\"base_summary\"],\n",
        "                row[\"lora_summary\"],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "print(f\"\\nSaved JSONL to: {JSONL_OUT}\")\n",
        "print(f\"Saved CSV to: {CSV_OUT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce RTX 5090\n",
            "Loaded 1500 test samples, using 10 for comparison.\n",
            "\n",
            "=== Generating base model summaries ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:1566: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4e64456c47945f3a029f78a96078760",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./lora_summarizer_llama3/full_model were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ./lora_summarizer_llama3/full_model and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Generating LoRA model summaries ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a303f20f456847bcaf034fddc2dedb77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./lora_summarizer_llama3/full_model were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ./lora_summarizer_llama3/full_model and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/workspace/venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:1566: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Example 0 =====\n",
            "Input (truncated):\n",
            "the current paradigm for the electronic structure of manganites ( a@xmath0b@xmath7mno@xmath2 ) with x @xmath8 1/2 is a lattice of mn sites with d@xmath9 or d@xmath10 orbital occupancy, with the proportion of each decided by the value of x and orbital ordering ( oo ) of the occupied e@xmath11 orbital on d@xmath10 sites. \n",
            " the corresponding double - exchange model hamiltonian has been studied extensively @xcite. \n",
            " manganites with x @xmath8 1/2 usually exhibit a phase transition @xcite which has be...\n",
            "\n",
            "Ground Truth:\n",
            "unrestricted hartree - fock calculations on la@xmath0ca@xmath1mno@xmath2 ( x = 0.5 and x = 0.67 ) in the full magnetic unit cell show that the magnetic ground states of these compounds consist of ferromagnetic molecules or polarons ordered in herring - bone patterns. \n",
            " each polaron consists of either two or three mn ions separated by o@xmath3 ions with a magnetic moment opposed to those of the mn ions. \n",
            " ferromagnetic coupling within the polarons is strong while coupling between them is relative...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 1 =====\n",
            "Input (truncated):\n",
            "modeling the core - collapse supernova ( sn ) mechanism in three dimensions ( 3d ) is still in its infancy. \n",
            " generalization from spherical symmetry ( 1d ) to axial symmetry ( 2d ) introduces nonradial flows and hydrodynamic instabilities like convection and the standing accretion shock instability ( `` sasi '' ; @xcite ) in the neutrino - heated postshock layer, which have been recognized as helpful for the explosion due to improved neutrino - heating conditions and buoyancy or turbulent pressu...\n",
            "\n",
            "Ground Truth:\n",
            "we present the first successful simulation of a neutrino - driven supernova explosion in three dimensions ( 3d ), using the prometheus - vertex code with an axis - free yin - yang grid and a sophisticated treatment of three - flavor, energy - dependent neutrino transport. \n",
            " the progenitor is a nonrotating, zero - metallicity 9.6@xmath0 star with an iron core. \n",
            " while in spherical symmetry outward shock acceleration sets in later than 300ms after bounce, a successful explosion starts at @xmath113...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 2 =====\n",
            "Input (truncated):\n",
            "a thorough understanding of the orbital structure in a barred galaxy potential can provide useful insight to the stellar dynamics of barred galaxies and therefore to the dynamical evolution of these objects, as reviewed e.g. by athanassoula ( 1984 ), contopoulos & grosbl ( 1989 ), sellwood & wilkinson ( 1993 ) and pfenniger ( 1996 ). \n",
            " stable periodic orbits trap around them regular orbits and thus constitute the backbone of galaxy structure ( athanassoula, bienayme, martinet et al. \n",
            " thus the a...\n",
            "\n",
            "Ground Truth:\n",
            "in this series of papers we investigate the orbital structure of 3d models representing barred galaxies. in the present introductory paper \n",
            " we use a fiducial case to describe all families of periodic orbits that may play a role in the morphology of three - dimensional bars. \n",
            " we show that, in a 3d bar, the backbone of the orbital structure is not just the x1 family, as in 2d models, but a tree of 2d and 3d families bifurcating from x1. besides the main tree we have also found another group of f...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 3 =====\n",
            "Input (truncated):\n",
            "lattice qcd plays an important role in understanding the n * spectrum. \n",
            " one can systematically study the spectrum sector by sector, with the ability to dial the quark masses, and dissect the degrees of freedom. \n",
            " the rich structure of the excited baryon spectrum, as tabulated by the particle data group @xcite, provides a fertile ground for exploring how the internal degrees of freedom in the nucleon are excited and how qcd works in a wider context. \n",
            " one outstanding example is the parity splitt...\n",
            "\n",
            "Ground Truth:\n",
            "we report n * masses in the spin-3/2 sector from a highly - improved anisotropic action. \n",
            " states with both positive and negative parity are isolated via a parity projection method. \n",
            " the extent to which spin projection is needed is examined. \n",
            " the gross features of the splittings from the nucleon ground state show a trend consistent with experimental results at the quark masses explored.\n",
            "\n",
            "Base Summary:\n",
            "testdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatestdatatest...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 4 =====\n",
            "Input (truncated):\n",
            "that local intergalactic space could harbor substantial amounts of hot, highly ionized gas, either as an extended halo surrounding the galaxy, or as an extended medium pervading the local group, was first suggested by spitzer ( 1956 ) and by kahn and woltjer ( 1959 ). \n",
            " such a medium would have a characteristic temperature of order the virial temperature, estimated to be on the order of @xmath8, and be of very low density, @xmath9 @xmath10 ( @xcite ). \n",
            " its emission, mainly soft thermal line emi...\n",
            "\n",
            "Ground Truth:\n",
            "recent observations with the dispersive x - ray spectrometers aboard _ \n",
            " chandra _ and _ newton observatory _ have begun to probe the properties of the x - ray intergalactic medium ( igm ) at small redshifts. using large quantities ( @xmath0950 ksec ) of spectroscopic data acquired using the reflection grating spectrometer ( rgs ) aboard _ newton observatory _, we investigated the intervening material toward three low redshift, high galactic latitude active galactic nuclei ( agns ) with nominall...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 5 =====\n",
            "Input (truncated):\n",
            "_ plasmodium falciparum _ malaria is a major parasitic disease which causes severe morbidity and mortality in approximately half a million people annually @xcite. \n",
            " artemisinin ( art ) and its derivatives ( e.g. artesunate, dihydroartemisinin and artemether ), used in combination with partner drugs, provide front - line protection, and have been responsible for dramatic reductions in disease burden over the past few decades @xcite. despite their clinical and public health effectiveness, the emer...\n",
            "\n",
            "Ground Truth:\n",
            "falciparum malaria is a major parasitic disease causing widespread morbidity and mortality globally. \n",
            " artemisinin derivatives the most effective and widely - used antimalarials that have helped reduce the burden of malaria by 60% in some areas over the past decade have recently been found to induce growth retardation of blood - stage _ plasmodium falciparum _ when applied at clinically relevant concentrations. to date, no model has been designed to quantify the growth retardation effect and to ...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 6 =====\n",
            "Input (truncated):\n",
            "the sensitivity of the observations of cosmic microwave background ( cmb ) anisotropies for existing and planned experiments will allow differentiation among theoretical models determining the parameters of cosmological interest with an uncertainty of only a few per cent. however, even the most sensitive experiments have poor signal to noise ratios ( hancock 1994 ; bennett 1996, etc. ) and are sensitive only to the most intense structures in the cmb signals. the statistical properties of the cmb...\n",
            "\n",
            "Ground Truth:\n",
            "even the most sensitive cosmic microwave background anisotropy experiments have signal to noise ratios @xmath0, so that an accurate determination of the properties of the cosmological signal requires a careful assessment of the experimental noise. \n",
            " most of the experiments combine simultaneous multi - channel observations in which the presence of correlated noise is likely. \n",
            " this case is common for ground - based experiments in which an important fraction of the noise could be atmospheric in or...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 7 =====\n",
            "Input (truncated):\n",
            "throughout this paper we work in the piecewise linear category. \n",
            " let @xmath4 be a link in the 3-sphere @xmath5 and @xmath2 a diagram of @xmath4 on the 2-sphere @xmath6. \n",
            " it is well known that by changing over / under information at some crossings of @xmath2 we have a diagram of a trivial link. \n",
            " let @xmath7 be the minimal number of such crossing changes. \n",
            " namely, there are some @xmath7 crossings of @xmath2 such that changing them yields a trivial link diagram, and changing less than @xmath7 c...\n",
            "\n",
            "Ground Truth:\n",
            "we show that for any nontrivial knot @xmath0 and any natural number @xmath1 there is a diagram @xmath2 of @xmath0 such that the unknotting number of @xmath2 is greater than or equal to @xmath1. \n",
            " it is well known that twice the unknotting number of @xmath0 is less than or equal to the crossing number of @xmath0 minus one. we show that the equality holds only when @xmath0 is a @xmath3-torus knot.\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 8 =====\n",
            "Input (truncated):\n",
            "until recently, information about the inter- and intra- molecular forces involved in the stability of the double stranded dna ( dsdna ) was obtained _ in vitro _ through indirect physical and thermodynamic measurements like nuclear magnetic resonance spectroscopy, light scattering, crystallography, differential scanning calorimetry etc. @xcite. \n",
            " such information is needed to understand two key biological processes, _ \n",
            " i.e. _, replication and transcription, where a dsdna is required to separate...\n",
            "\n",
            "Ground Truth:\n",
            "we have studied the separation of a double stranded dna ( dsdna ), which is driven either by the temperature or force. by monitoring the probability of opening of entire base pairs along the chain, \n",
            " we show that the opening of a dsdna depends not only on the sequence but also on the constraints on the chain in the experimental setups. \n",
            " our results clearly demonstrate that the force induced melting of dsdna, whose one of the ends is constrained, is significantly different from the thermal melti...\n",
            "\n",
            "Base Summary:\n",
            "OPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLEOPLE...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "===== Example 9 =====\n",
            "Input (truncated):\n",
            "lipid bilayers are very interesting 2d model systems displaying complex behaviours @xcite ; they are also one of the basic building blocks of biological systems @xcite, which explains why they have been the object of growing interest over the past decades. when studying the physical properties of lipid membranes, two kinds of ideal systems \n",
            " are commonly used : planar membranes supported on hydrophilic substrates, which well - defined position enables the use of many experimental techniques, and...\n",
            "\n",
            "Ground Truth:\n",
            "we report results concerning the destabilisation of supported phospholipid bilayers in a well - defined geometry. \n",
            " when heating up supported phospholipid membranes deposited on highly hydrophilic glass slides from room temperature ( i.e. with lipids in the gel phase ), unbinding was observed around the main gel to fluid transition temperature of the lipids. \n",
            " it lead to the formation of relatively monodisperse vesicles, of which most remained tethered to the supported bilayer. \n",
            " we interpret th...\n",
            "\n",
            "Base Summary:\n",
            "timeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeofdaytimeo...\n",
            "\n",
            "LoRA Summary:\n",
            "://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://:/...\n",
            "================================================================================\n",
            "\n",
            "Saved JSONL to: ./outputs/comparisons.jsonl\n",
            "Saved CSV to: ./outputs/comparisons.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "DATA_DIR = \"./data\"\n",
        "TEST_PATH = os.path.join(DATA_DIR, \"test.jsonl\")\n",
        "\n",
        "BASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"   # same base model as training\n",
        "BASE_MODEL_DIR = \"./lora_summarizer_llama3/full_model\"\n",
        "LORA_ADAPTER_DIR = \"./lora_summarizer_llama3/lora_adapter\"\n",
        "\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "JSONL_OUT = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "CSV_OUT = os.path.join(OUTPUT_DIR, \"comparisons.csv\")\n",
        "\n",
        "# Use smaller context at inference to avoid OOM\n",
        "max_input_length = 2048\n",
        "max_new_tokens = 256\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA GPU not available. Check your environment.\"\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ----------------------\n",
        "# Load test set and sample 10 examples\n",
        "# ----------------------\n",
        "test_records = []\n",
        "with open(TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        test_records.append(json.loads(line))\n",
        "\n",
        "assert len(test_records) > 0, \"Test set is empty.\"\n",
        "samples = random.sample(test_records, k=min(10, len(test_records)))\n",
        "print(f\"Loaded {len(test_records)} test samples, using {len(samples)} for comparison.\")\n",
        "\n",
        "# ----------------------\n",
        "# Tokenizer (from base model name, not local dir)\n",
        "# ----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# ----------------------\n",
        "# Generation helper\n",
        "# ----------------------\n",
        "def generate_summary(model, input_text: str) -> str:\n",
        "    prompt = (\n",
        "        \"Summarize the following scientific article.\\n\\n\"\n",
        "        \"Article:\\n\"\n",
        "        f\"{input_text}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    summary = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "    return summary\n",
        "\n",
        "# ----------------------\n",
        "# First pass: base model summaries (only base model in GPU)\n",
        "# ----------------------\n",
        "print(\"\\n=== Generating base model summaries ===\")\n",
        "base_summaries = []\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_DIR,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.eval()\n",
        "\n",
        "for idx, ex in enumerate(samples):\n",
        "    input_text = ex[\"input_text\"]\n",
        "    base_summary = generate_summary(base_model, input_text)\n",
        "    base_summaries.append(base_summary)\n",
        "\n",
        "# Free GPU memory used by base model\n",
        "del base_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ----------------------\n",
        "# Second pass: LoRA model summaries\n",
        "# ----------------------\n",
        "print(\"\\n=== Generating LoRA model summaries ===\")\n",
        "lora_summaries = []\n",
        "\n",
        "lora_base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_DIR,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "lora_model = PeftModel.from_pretrained(lora_base, LORA_ADAPTER_DIR)\n",
        "lora_model.eval()\n",
        "\n",
        "for idx, ex in enumerate(samples):\n",
        "    input_text = ex[\"input_text\"]\n",
        "    lora_summary = generate_summary(lora_model, input_text)\n",
        "    lora_summaries.append(lora_summary)\n",
        "\n",
        "# Free GPU memory used by LoRA model (optional)\n",
        "del lora_model\n",
        "del lora_base\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ----------------------\n",
        "# Collect results, print side-by-side, and save\n",
        "# ----------------------\n",
        "results = []\n",
        "for i, ex in enumerate(samples):\n",
        "    input_text = ex[\"input_text\"]\n",
        "    ground_truth = ex[\"target_text\"]\n",
        "    base_summary = base_summaries[i]\n",
        "    lora_summary = lora_summaries[i]\n",
        "\n",
        "    print(f\"\\n===== Example {i} =====\")\n",
        "    print(\"Input (truncated):\")\n",
        "    print(input_text[:500] + (\"...\" if len(input_text) > 500 else \"\"))\n",
        "    print(\"\\nGround Truth:\")\n",
        "    print(ground_truth[:500] + (\"...\" if len(ground_truth) > 500 else \"\"))\n",
        "    print(\"\\nBase Summary:\")\n",
        "    print(base_summary[:500] + (\"...\" if len(base_summary) > 500 else \"\"))\n",
        "    print(\"\\nLoRA Summary:\")\n",
        "    print(lora_summary[:500] + (\"...\" if len(lora_summary) > 500 else \"\"))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    results.append(\n",
        "        {\n",
        "            \"id\": i,\n",
        "            \"input_text\": input_text,\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"base_summary\": base_summary,\n",
        "            \"lora_summary\": lora_summary,\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save to JSONL\n",
        "with open(JSONL_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in results:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Save to CSV\n",
        "with open(CSV_OUT, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"id\", \"input_text\", \"ground_truth\", \"base_summary\", \"lora_summary\"])\n",
        "    for row in results:\n",
        "        writer.writerow(\n",
        "            [\n",
        "                row[\"id\"],\n",
        "                row[\"input_text\"],\n",
        "                row[\"ground_truth\"],\n",
        "                row[\"base_summary\"],\n",
        "                row[\"lora_summary\"],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "print(f\"\\nSaved JSONL to: {JSONL_OUT}\")\n",
        "print(f\"Saved CSV to: {CSV_OUT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Loaded 10 comparison records.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bad1281bc0741f1b8c83f7611282fd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Metric Summary (mean  std) ===\n",
            "Metric             Base_mean     Base_std    LoRA_mean     LoRA_std\n",
            "rouge1                0.0000       0.0000       0.0000       0.0000\n",
            "rouge2                0.0000       0.0000       0.0000       0.0000\n",
            "rougeL                0.0000       0.0000       0.0000       0.0000\n",
            "bleu                  0.0000       0.0000       0.0000       0.0000\n",
            "bertscore_f1          0.6800       0.0069       0.6866       0.0132\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved metrics summary to: ./outputs/metrics_summary.json\n"
          ]
        }
      ],
      "source": [
        "# Part 4.A  Automatic Evaluation: ROUGE, BLEU, BERTScore + plots\n",
        "\n",
        "%pip install -q evaluate rouge-score nltk bert-score matplotlib\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import evaluate\n",
        "import torch\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "# ----------------------\n",
        "# Config & paths\n",
        "# ----------------------\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "COMPARISONS_PATH = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "METRICS_SUMMARY_PATH = os.path.join(OUTPUT_DIR, \"metrics_summary.json\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "random.seed(42)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ----------------------\n",
        "# Load comparisons.jsonl\n",
        "# ----------------------\n",
        "records = []\n",
        "with open(COMPARISONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "assert len(records) > 0, \"No records found in comparisons.jsonl\"\n",
        "\n",
        "ground_truths = [r[\"ground_truth\"] for r in records]\n",
        "base_summaries = [r[\"base_summary\"] for r in records]\n",
        "lora_summaries = [r[\"lora_summary\"] for r in records]\n",
        "\n",
        "print(f\"Loaded {len(records)} comparison records.\")\n",
        "\n",
        "# ----------------------\n",
        "# ROUGE (using evaluate)\n",
        "# ----------------------\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "rouge_base = rouge_metric.compute(\n",
        "    predictions=base_summaries,\n",
        "    references=ground_truths,\n",
        "    use_aggregator=False,\n",
        ")\n",
        "rouge_lora = rouge_metric.compute(\n",
        "    predictions=lora_summaries,\n",
        "    references=ground_truths,\n",
        "    use_aggregator=False,\n",
        ")\n",
        "\n",
        "# rouge_base / rouge_lora keys: 'rouge1', 'rouge2', 'rougeL', 'rougeLsum'\n",
        "# We will use rouge1, rouge2, rougeL\n",
        "rouge_metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
        "\n",
        "# ----------------------\n",
        "# BLEU (sentence-level with smoothing)\n",
        "# ----------------------\n",
        "smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "def compute_bleu_list(preds, refs):\n",
        "    scores = []\n",
        "    for hyp, ref in zip(preds, refs):\n",
        "        hyp_tokens = hyp.split()\n",
        "        ref_tokens = ref.split()\n",
        "        if len(hyp_tokens) == 0 or len(ref_tokens) == 0:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        scores.append(\n",
        "            sentence_bleu(\n",
        "                [ref_tokens],\n",
        "                hyp_tokens,\n",
        "                smoothing_function=smooth_fn,\n",
        "            )\n",
        "        )\n",
        "    return scores\n",
        "\n",
        "bleu_base = compute_bleu_list(base_summaries, ground_truths)\n",
        "bleu_lora = compute_bleu_list(lora_summaries, ground_truths)\n",
        "\n",
        "# ----------------------\n",
        "# BERTScore (F1 only)\n",
        "# ----------------------\n",
        "device = \"cpu\"\n",
        "\n",
        "P_b, R_b, F1_b = bert_score(\n",
        "    base_summaries,\n",
        "    ground_truths,\n",
        "    lang=\"en\",\n",
        "    device=device,\n",
        "    verbose=False,\n",
        ")\n",
        "P_l, R_l, F1_l = bert_score(\n",
        "    lora_summaries,\n",
        "    ground_truths,\n",
        "    lang=\"en\",\n",
        "    device=device,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "bertscore_f1_base = F1_b.tolist()\n",
        "bertscore_f1_lora = F1_l.tolist()\n",
        "\n",
        "# ----------------------\n",
        "# Aggregate metrics: mean & std\n",
        "# ----------------------\n",
        "def mean_std(values):\n",
        "    arr = np.array(values, dtype=float)\n",
        "    return float(arr.mean()), float(arr.std(ddof=0))\n",
        "\n",
        "metrics_summary = {}\n",
        "\n",
        "# ROUGE\n",
        "for m in rouge_metrics:\n",
        "    base_vals = rouge_base[m]\n",
        "    lora_vals = rouge_lora[m]\n",
        "    b_mean, b_std = mean_std(base_vals)\n",
        "    l_mean, l_std = mean_std(lora_vals)\n",
        "    metrics_summary[m] = {\n",
        "        \"base\": {\"mean\": b_mean, \"std\": b_std},\n",
        "        \"lora\": {\"mean\": l_mean, \"std\": l_std},\n",
        "    }\n",
        "\n",
        "# BLEU\n",
        "b_mean, b_std = mean_std(bleu_base)\n",
        "l_mean, l_std = mean_std(bleu_lora)\n",
        "metrics_summary[\"bleu\"] = {\n",
        "    \"base\": {\"mean\": b_mean, \"std\": b_std},\n",
        "    \"lora\": {\"mean\": l_mean, \"std\": l_std},\n",
        "}\n",
        "\n",
        "# BERTScore F1\n",
        "b_mean, b_std = mean_std(bertscore_f1_base)\n",
        "l_mean, l_std = mean_std(bertscore_f1_lora)\n",
        "metrics_summary[\"bertscore_f1\"] = {\n",
        "    \"base\": {\"mean\": b_mean, \"std\": b_std},\n",
        "    \"lora\": {\"mean\": l_mean, \"std\": l_std},\n",
        "}\n",
        "\n",
        "# ----------------------\n",
        "# Print numeric table\n",
        "# ----------------------\n",
        "print(\"\\n=== Metric Summary (mean  std) ===\")\n",
        "print(f\"{'Metric':<15} {'Base_mean':>12} {'Base_std':>12} {'LoRA_mean':>12} {'LoRA_std':>12}\")\n",
        "for metric_name, vals in metrics_summary.items():\n",
        "    b = vals[\"base\"]\n",
        "    l = vals[\"lora\"]\n",
        "    print(\n",
        "        f\"{metric_name:<15} \"\n",
        "        f\"{b['mean']:>12.4f} {b['std']:>12.4f} \"\n",
        "        f\"{l['mean']:>12.4f} {l['std']:>12.4f}\"\n",
        "    )\n",
        "\n",
        "# ----------------------\n",
        "# Bar plots (Base vs LoRA) for each metric\n",
        "# ----------------------\n",
        "def plot_metric(metric_key, title=None, ylabel=None):\n",
        "    base_mean = metrics_summary[metric_key][\"base\"][\"mean\"]\n",
        "    base_std = metrics_summary[metric_key][\"base\"][\"std\"]\n",
        "    lora_mean = metrics_summary[metric_key][\"lora\"][\"mean\"]\n",
        "    lora_std = metrics_summary[metric_key][\"lora\"][\"std\"]\n",
        "\n",
        "    labels = [\"Base\", \"LoRA\"]\n",
        "    means = [base_mean, lora_mean]\n",
        "    stds = [base_std, lora_std]\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.bar(labels, means, yerr=stds, capsize=5, color=[\"gray\", \"steelblue\"])\n",
        "    plt.title(title or metric_key)\n",
        "    plt.ylabel(ylabel or \"Score\")\n",
        "    plt.ylim(0, max(means) * 1.2 if max(means) > 0 else 1.0)\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(\"rouge1\", title=\"ROUGE-1 (Base vs LoRA)\", ylabel=\"F1\")\n",
        "plot_metric(\"rouge2\", title=\"ROUGE-2 (Base vs LoRA)\", ylabel=\"F1\")\n",
        "plot_metric(\"rougeL\", title=\"ROUGE-L (Base vs LoRA)\", ylabel=\"F1\")\n",
        "plot_metric(\"bleu\", title=\"BLEU (Base vs LoRA)\", ylabel=\"BLEU score\")\n",
        "plot_metric(\"bertscore_f1\", title=\"BERTScore F1 (Base vs LoRA)\", ylabel=\"F1\")\n",
        "\n",
        "# ----------------------\n",
        "# Save metrics_summary.json\n",
        "# ----------------------\n",
        "with open(METRICS_SUMMARY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved metrics summary to: {METRICS_SUMMARY_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GEMINI_API_KEY=\"AIza****************************\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Part 4.B  LLM-as-a-Judge using Gemini 2.5 Flash (instead of Together.ai)\n",
        "# - Evaluates 10 samples (base vs LoRA) on Fluency, Factuality, Coverage\n",
        "# - Saves ./outputs/llm_judge_scores_gemini.json\n",
        "\n",
        "%pip install -q google-generativeai\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "COMPARISONS_PATH = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "LLM_JUDGE_OUT = os.path.join(OUTPUT_DIR, \"llm_judge_scores_gemini.json\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "GEMINI_MODEL_NAME = \"gemini-2.5-flash\"       # LLM-as-a-Judge model\n",
        "\n",
        "MAX_INPUT_CHARS = 4000  # trim long texts for the judge\n",
        "NUM_SAMPLES = 10\n",
        "random.seed(42)\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "judge_model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "\n",
        "# ----------------------\n",
        "# Exact LLM-as-a-Judge prompt (SYSTEM + USER) with placeholders\n",
        "# ----------------------\n",
        "JUDGE_PROMPT_TEMPLATE = \"\"\"SYSTEM: You are a strict evaluator. Rate the generated summary along three dimensions from 1 (poor) to 5 (excellent). Provide only a JSON object as the final output.\n",
        "\n",
        "USER:\n",
        "\n",
        "Input: <<INPUT_TEXT>>\n",
        "\n",
        "Ground-Truth Abstract: <<GROUND_TRUTH>>\n",
        "\n",
        "Generated Summary: <<GENERATED_SUMMARY>>\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- For each dimension (fluency, factuality, coverage) return:\n",
        "\n",
        "  - \"score\": integer 1-5\n",
        "\n",
        "  - \"justification\": a short justification (max 40 words)\n",
        "\n",
        "  - \"errors\": a short list (max 3 items) describing factual errors or hallucinations; if none, return [].\n",
        "\n",
        "- Return a single valid JSON object with keys: \"fluency\", \"factuality\", \"coverage\".\n",
        "\n",
        "- Example output:\n",
        "\n",
        "{\n",
        "  \"fluency\": {\"score\": 4, \"justification\": \"Readable and grammatical\", \"errors\": []},\n",
        "  \"factuality\": {\"score\": 3, \"justification\": \"Missed experimental detail\", \"errors\": [\"Says dataset size is 1.5k rather than 15k\"]},\n",
        "  \"coverage\": {\"score\": 2, \"justification\": \"Omits method description\", \"errors\": [\"No mention of optimization method\"]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def truncate_text(text: str, max_chars: int = MAX_INPUT_CHARS) -> str:\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars] + \"\\n\\n(TRUNCATED)\"\n",
        "\n",
        "def build_judge_prompt(input_text: str, ground_truth: str, generated_summary: str) -> str:\n",
        "    return (\n",
        "        JUDGE_PROMPT_TEMPLATE\n",
        "        .replace(\"<<INPUT_TEXT>>\", truncate_text(input_text))\n",
        "        .replace(\"<<GROUND_TRUTH>>\", truncate_text(ground_truth))\n",
        "        .replace(\"<<GENERATED_SUMMARY>>\", truncate_text(generated_summary))\n",
        "    )\n",
        "\n",
        "# ----------------------\n",
        "# Single Gemini call for one judgment\n",
        "# ----------------------\n",
        "def call_gemini_judge(input_text: str,\n",
        "                      ground_truth: str,\n",
        "                      generated_summary: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calls Gemini 2.5 Flash with the specified SYSTEM/USER-style prompt.\n",
        "    Returns parsed JSON dict with keys: fluency, factuality, coverage.\n",
        "    If Gemini returns no text (e.g., due to safety), returns a neutral fallback.\n",
        "    \"\"\"\n",
        "    prompt = build_judge_prompt(input_text, ground_truth, generated_summary)\n",
        "\n",
        "    response = judge_model.generate_content(\n",
        "        prompt,\n",
        "        generation_config={\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_output_tokens\": 512,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Helper: neutral fallback if model gives no usable text\n",
        "    def fallback_judgment(reason: str = \"no_output\"):\n",
        "        return {\n",
        "            \"fluency\":   {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"factuality\":{\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"coverage\":  {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "        }\n",
        "\n",
        "    # Safely extract text from candidates without using response.text\n",
        "    if not getattr(response, \"candidates\", None):\n",
        "        return fallback_judgment(\"no_candidates\")\n",
        "\n",
        "    candidate = response.candidates[0]\n",
        "    parts = getattr(candidate, \"content\", candidate).parts if hasattr(candidate, \"content\") else []\n",
        "\n",
        "    texts = []\n",
        "    for p in parts:\n",
        "        t = getattr(p, \"text\", None)\n",
        "        if t:\n",
        "            texts.append(t)\n",
        "    content = \"\\n\".join(texts).strip()\n",
        "\n",
        "    if not content:\n",
        "        # Nothing usable produced (likely safety/max_tokens), return fallback\n",
        "        return fallback_judgment(\"empty_content\")\n",
        "\n",
        "    # Robust JSON extraction: substring between first '{' and last '}'\n",
        "    try:\n",
        "        start = content.index(\"{\")\n",
        "        end = content.rindex(\"}\") + 1\n",
        "        json_str = content[start:end]\n",
        "    except ValueError:\n",
        "        json_str = content  # fallback to entire content\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str_fixed = json_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        try:\n",
        "            parsed = json.loads(json_str_fixed)\n",
        "        except json.JSONDecodeError:\n",
        "            # If still broken, return fallback\n",
        "            return fallback_judgment(\"json_parse_error\")\n",
        "\n",
        "    return parsed\n",
        "# ----------------------\n",
        "# Run evaluation over 10 samples (base vs LoRA)\n",
        "# ----------------------\n",
        "# Safer version: handles API errors / quota limits and still saves partial results\n",
        "\n",
        "# Stateful / resumable Gemini LLM-as-a-Judge evaluation:\n",
        "# - Reuses existing ./outputs/llm_judge_scores_gemini.json if present\n",
        "# - Only calls Gemini for samples that have NOT been evaluated yet\n",
        "\n",
        "def run_gemini_llm_judge_evaluation():\n",
        "    \"\"\"\n",
        "    Runs LLM-as-a-Judge over up to NUM_SAMPLES examples.\n",
        "    - If llm_judge_scores_gemini.json exists, reuses previously evaluated samples.\n",
        "    - Only sends Gemini requests for remaining samples.\n",
        "    - On any API error, stops further calls but still saves all accumulated results.\n",
        "    \"\"\"\n",
        "    # Load comparisons.jsonl\n",
        "    records = []\n",
        "    with open(COMPARISONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            # Ensure each record has a stable id\n",
        "            if \"id\" not in rec:\n",
        "                rec[\"id\"] = idx\n",
        "            records.append(rec)\n",
        "\n",
        "    if len(records) == 0:\n",
        "        raise RuntimeError(\"No records found in comparisons.jsonl\")\n",
        "\n",
        "    # Map id -> record\n",
        "    id_to_rec = {rec[\"id\"]: rec for rec in records}\n",
        "    all_ids = sorted(id_to_rec.keys())\n",
        "    target_ids = all_ids[:min(NUM_SAMPLES, len(all_ids))]\n",
        "\n",
        "    print(f\"Target IDs for evaluation (up to {NUM_SAMPLES}): {target_ids}\")\n",
        "\n",
        "    # ----------------------\n",
        "    # Load existing results if file exists\n",
        "    # ----------------------\n",
        "    all_scores = {\n",
        "        \"base\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "        \"lora\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "    }\n",
        "    per_sample = []\n",
        "    evaluated_ids = set()\n",
        "\n",
        "    if os.path.exists(LLM_JUDGE_OUT):\n",
        "        try:\n",
        "            with open(LLM_JUDGE_OUT, \"r\", encoding=\"utf-8\") as f:\n",
        "                existing = json.load(f)\n",
        "            for entry in existing.get(\"per_sample\", []):\n",
        "                rid = entry.get(\"id\")\n",
        "                if rid in target_ids:\n",
        "                    per_sample.append(entry)\n",
        "                    evaluated_ids.add(rid)\n",
        "                    for which in [\"base\", \"lora\"]:\n",
        "                        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                            score = int(entry[which][dim][\"score\"])\n",
        "                            all_scores[which][dim].append(score)\n",
        "            print(f\"Loaded existing judgments for IDs: {sorted(evaluated_ids)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Failed to load existing {LLM_JUDGE_OUT}: {e}\")\n",
        "    else:\n",
        "        print(\"No existing llm_judge_scores_gemini.json found; starting fresh.\")\n",
        "\n",
        "    remaining_ids = [rid for rid in target_ids if rid not in evaluated_ids]\n",
        "    print(f\"Remaining IDs to evaluate: {remaining_ids}\")\n",
        "\n",
        "    # ----------------------\n",
        "    # Evaluate remaining samples\n",
        "    # ----------------------\n",
        "    for rid in remaining_ids:\n",
        "        rec = id_to_rec[rid]\n",
        "        input_text = rec[\"input_text\"]\n",
        "        ground_truth = rec[\"ground_truth\"]\n",
        "        base_summary = rec[\"base_summary\"]\n",
        "        lora_summary = rec[\"lora_summary\"]\n",
        "\n",
        "        print(f\"\\n=== Evaluating id={rid} ===\")\n",
        "        try:\n",
        "            print(\"Calling Gemini judge for BASE summary...\")\n",
        "            base_judgment = call_gemini_judge(input_text, ground_truth, base_summary)\n",
        "\n",
        "            print(\"Calling Gemini judge for LoRA summary...\")\n",
        "            lora_judgment = call_gemini_judge(input_text, ground_truth, lora_summary)\n",
        "\n",
        "            # Parse and store scores\n",
        "            for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                b_score = int(base_judgment[dim][\"score\"])\n",
        "                l_score = int(lora_judgment[dim][\"score\"])\n",
        "                all_scores[\"base\"][dim].append(b_score)\n",
        "                all_scores[\"lora\"][dim].append(l_score)\n",
        "\n",
        "            per_sample.append(\n",
        "                {\n",
        "                    \"id\": rid,\n",
        "                    \"base\": base_judgment,\n",
        "                    \"lora\": lora_judgment,\n",
        "                }\n",
        "            )\n",
        "            evaluated_ids.add(rid)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Stopping evaluation due to error on id={rid}: {e}\")\n",
        "            break\n",
        "\n",
        "    evaluated_n = len(evaluated_ids)\n",
        "    if evaluated_n == 0:\n",
        "        print(\"\\nNo samples were successfully evaluated; nothing to save.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFinished with {evaluated_n} successfully evaluated unique IDs.\")\n",
        "\n",
        "    # ----------------------\n",
        "    # Aggregate averages\n",
        "    # ----------------------\n",
        "    summary = {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"model\": GEMINI_MODEL_NAME,\n",
        "        \"num_samples_requested\": len(target_ids),\n",
        "        \"num_samples_evaluated\": evaluated_n,\n",
        "        \"evaluated_ids\": sorted(evaluated_ids),\n",
        "        \"per_sample\": per_sample,\n",
        "        \"avg\": {},\n",
        "    }\n",
        "\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        summary[\"avg\"][which] = {}\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            scores = all_scores[which][dim]\n",
        "            if scores:\n",
        "                mean_val = float(statistics.mean(scores))\n",
        "                std_val = float(statistics.pstdev(scores)) if len(scores) > 1 else 0.0\n",
        "            else:\n",
        "                mean_val, std_val = 0.0, 0.0\n",
        "            summary[\"avg\"][which][dim] = {\n",
        "                \"mean\": mean_val,\n",
        "                \"std\": std_val,\n",
        "                \"scores\": scores,\n",
        "            }\n",
        "\n",
        "    # ----------------------\n",
        "    # Save JSON\n",
        "    # ----------------------\n",
        "    with open(LLM_JUDGE_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved Gemini LLM-as-a-Judge scores to: {LLM_JUDGE_OUT}\")\n",
        "    print(\"Average scores (mean over evaluated samples):\")\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        print(f\"\\n{which.upper()}:\")\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            stats_dim = summary[\"avg\"][which][dim]\n",
        "            print(\n",
        "                f\"  {dim}: mean={stats_dim['mean']:.2f}, \"\n",
        "                f\"std={stats_dim['std']:.2f}, scores={stats_dim['scores']}\"\n",
        "            )\n",
        "\n",
        "# ----------------------\n",
        "# Example: run the evaluation once API key is set\n",
        "# ----------------------\n",
        "# run_gemini_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target IDs for evaluation (up to 10): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Loaded existing judgments for IDs: [0, 1, 4, 6, 9]\n",
            "Remaining IDs to evaluate: [2, 3, 5, 7, 8]\n",
            "\n",
            "=== Evaluating id=2 ===\n",
            "Calling Gemini judge for BASE summary...\n",
            "Calling Gemini judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=3 ===\n",
            "Calling Gemini judge for BASE summary...\n",
            "Calling Gemini judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=5 ===\n",
            "Calling Gemini judge for BASE summary...\n",
            "Calling Gemini judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=7 ===\n",
            "Calling Gemini judge for BASE summary...\n",
            "Calling Gemini judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=8 ===\n",
            "Calling Gemini judge for BASE summary...\n",
            "Calling Gemini judge for LoRA summary...\n",
            "\n",
            "Finished with 10 successfully evaluated unique IDs.\n",
            "\n",
            "Saved Gemini LLM-as-a-Judge scores to: ./outputs/llm_judge_scores_gemini.json\n",
            "Average scores (mean over evaluated samples):\n",
            "\n",
            "BASE:\n",
            "  fluency: mean=3.00, std=0.00, scores=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "  factuality: mean=3.00, std=0.00, scores=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "  coverage: mean=3.00, std=0.00, scores=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "\n",
            "LORA:\n",
            "  fluency: mean=3.00, std=0.00, scores=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "  factuality: mean=3.00, std=0.00, scores=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "  coverage: mean=3.00, std=0.00, scores=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ],
      "source": [
        "run_gemini_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Part 4.B  LLM-as-a-Judge using Groq (llama-3.3-70b-versatile)\n",
        "# - Evaluates up to NUM_SAMPLES samples (base vs LoRA)\n",
        "# - Saves ./outputs/llm_judge_scores_groq.json\n",
        "# - Resumable: reuses previous results and only judges missing ids\n",
        "\n",
        "%pip install -q groq\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "from groq import Groq\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "COMPARISONS_PATH = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "LLM_JUDGE_OUT = os.path.join(OUTPUT_DIR, \"llm_judge_scores_groq.json\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "GROQ_MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "NUM_SAMPLES = 10\n",
        "MAX_INPUT_CHARS = 4000\n",
        "random.seed(42)\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# ----------------------\n",
        "# Exact LLM-as-a-Judge prompt (SYSTEM + USER) as specified\n",
        "# ----------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a strict evaluator. Rate the generated summary along three dimensions from 1 (poor) \"\n",
        "    \"to 5 (excellent). Provide only a JSON object as the final output.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Input: <<INPUT_TEXT>>\n",
        "\n",
        "Ground-Truth Abstract: <<GROUND_TRUTH>>\n",
        "\n",
        "Generated Summary: <<GENERATED_SUMMARY>>\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- For each dimension (fluency, factuality, coverage) return:\n",
        "\n",
        "  - \"score\": integer 1-5\n",
        "\n",
        "  - \"justification\": a short justification (max 40 words)\n",
        "\n",
        "  - \"errors\": a short list (max 3 items) describing factual errors or hallucinations; if none, return [].\n",
        "\n",
        "- Return a single valid JSON object with keys: \"fluency\", \"factuality\", \"coverage\".\n",
        "\n",
        "- Example output:\n",
        "\n",
        "{\n",
        "  \"fluency\": {\"score\": 4, \"justification\": \"Readable and grammatical\", \"errors\": []},\n",
        "  \"factuality\": {\"score\": 3, \"justification\": \"Missed experimental detail\", \"errors\": [\"Says dataset size is 1.5k rather than 15k\"]},\n",
        "  \"coverage\": {\"score\": 2, \"justification\": \"Omits method description\", \"errors\": [\"No mention of optimization method\"]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def truncate_text(text: str, max_chars: int = MAX_INPUT_CHARS) -> str:\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars] + \"\\n\\n(TRUNCATED)\"\n",
        "\n",
        "def build_user_prompt(input_text: str, ground_truth: str, generated_summary: str) -> str:\n",
        "    return (\n",
        "        USER_PROMPT_TEMPLATE\n",
        "        .replace(\"<<INPUT_TEXT>>\", truncate_text(input_text))\n",
        "        .replace(\"<<GROUND_TRUTH>>\", truncate_text(ground_truth))\n",
        "        .replace(\"<<GENERATED_SUMMARY>>\", truncate_text(generated_summary))\n",
        "    )\n",
        "\n",
        "# ----------------------\n",
        "# Single Groq call for one judgment\n",
        "# ----------------------\n",
        "def call_groq_judge(input_text: str,\n",
        "                    ground_truth: str,\n",
        "                    generated_summary: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calls Groq (llama-3.3-70b-versatile) with the SYSTEM/USER prompt.\n",
        "    Returns parsed JSON dict with keys: fluency, factuality, coverage.\n",
        "    On bad JSON or empty content, returns a neutral fallback.\n",
        "    \"\"\"\n",
        "    user_prompt = build_user_prompt(input_text, ground_truth, generated_summary)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=GROQ_MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_completion_tokens=512,\n",
        "        top_p=1.0,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    def fallback_judgment(reason: str = \"no_output\"):\n",
        "        return {\n",
        "            \"fluency\":   {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"factuality\":{\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"coverage\":  {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "        }\n",
        "\n",
        "    if not completion.choices:\n",
        "        return fallback_judgment(\"no_choices\")\n",
        "\n",
        "    content = (completion.choices[0].message.content or \"\").strip()\n",
        "    if not content:\n",
        "        return fallback_judgment(\"empty_content\")\n",
        "\n",
        "    # Extract JSON between first { and last }\n",
        "    try:\n",
        "        start = content.index(\"{\")\n",
        "        end = content.rindex(\"}\") + 1\n",
        "        json_str = content[start:end]\n",
        "    except ValueError:\n",
        "        json_str = content\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str_fixed = json_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        try:\n",
        "            parsed = json.loads(json_str_fixed)\n",
        "        except json.JSONDecodeError:\n",
        "            return fallback_judgment(\"json_parse_error\")\n",
        "\n",
        "    return parsed\n",
        "\n",
        "# ----------------------\n",
        "# Resumable evaluation over up to NUM_SAMPLES samples\n",
        "# ----------------------\n",
        "def run_groq_llm_judge_evaluation():\n",
        "    \"\"\"\n",
        "    Runs LLM-as-a-Judge over up to NUM_SAMPLES examples.\n",
        "    - If llm_judge_scores_groq.json exists, reuses previous results.\n",
        "    - Only calls Groq for remaining ids.\n",
        "    - On API error, stops and still saves accumulated results.\n",
        "    \"\"\"\n",
        "    # Load comparisons.jsonl\n",
        "    records = []\n",
        "    with open(COMPARISONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            if \"id\" not in rec:\n",
        "                rec[\"id\"] = idx\n",
        "            records.append(rec)\n",
        "\n",
        "    if not records:\n",
        "        raise RuntimeError(\"No records found in comparisons.jsonl\")\n",
        "\n",
        "    id_to_rec = {rec[\"id\"]: rec for rec in records}\n",
        "    all_ids = sorted(id_to_rec.keys())\n",
        "    target_ids = all_ids[:min(NUM_SAMPLES, len(all_ids))]\n",
        "\n",
        "    print(f\"Target IDs for evaluation (up to {NUM_SAMPLES}): {target_ids}\")\n",
        "\n",
        "    # Prepare containers\n",
        "    all_scores = {\n",
        "        \"base\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "        \"lora\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "    }\n",
        "    per_sample = []\n",
        "    evaluated_ids = set()\n",
        "\n",
        "    # Load existing results if any\n",
        "    if os.path.exists(LLM_JUDGE_OUT):\n",
        "        try:\n",
        "            with open(LLM_JUDGE_OUT, \"r\", encoding=\"utf-8\") as f:\n",
        "                existing = json.load(f)\n",
        "            for entry in existing.get(\"per_sample\", []):\n",
        "                rid = entry.get(\"id\")\n",
        "                if rid in target_ids:\n",
        "                    per_sample.append(entry)\n",
        "                    evaluated_ids.add(rid)\n",
        "                    for which in [\"base\", \"lora\"]:\n",
        "                        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                            score = int(entry[which][dim][\"score\"])\n",
        "                            all_scores[which][dim].append(score)\n",
        "            print(f\"Loaded existing judgments for IDs: {sorted(evaluated_ids)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Failed to load existing {LLM_JUDGE_OUT}: {e}\")\n",
        "    else:\n",
        "        print(\"No existing llm_judge_scores_groq.json found; starting fresh.\")\n",
        "\n",
        "    remaining_ids = [rid for rid in target_ids if rid not in evaluated_ids]\n",
        "    print(f\"Remaining IDs to evaluate: {remaining_ids}\")\n",
        "\n",
        "    # Evaluate remaining\n",
        "    for rid in remaining_ids:\n",
        "        rec = id_to_rec[rid]\n",
        "        input_text = rec[\"input_text\"]\n",
        "        ground_truth = rec[\"ground_truth\"]\n",
        "        base_summary = rec[\"base_summary\"]\n",
        "        lora_summary = rec[\"lora_summary\"]\n",
        "\n",
        "        print(f\"\\n=== Evaluating id={rid} ===\")\n",
        "        try:\n",
        "            print(\"Calling Groq judge for BASE summary...\")\n",
        "            base_judgment = call_groq_judge(input_text, ground_truth, base_summary)\n",
        "\n",
        "            print(\"Calling Groq judge for LoRA summary...\")\n",
        "            lora_judgment = call_groq_judge(input_text, ground_truth, lora_summary)\n",
        "\n",
        "            for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                b_score = int(base_judgment[dim][\"score\"])\n",
        "                l_score = int(lora_judgment[dim][\"score\"])\n",
        "                all_scores[\"base\"][dim].append(b_score)\n",
        "                all_scores[\"lora\"][dim].append(l_score)\n",
        "\n",
        "            per_sample.append(\n",
        "                {\n",
        "                    \"id\": rid,\n",
        "                    \"base\": base_judgment,\n",
        "                    \"lora\": lora_judgment,\n",
        "                }\n",
        "            )\n",
        "            evaluated_ids.add(rid)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Stopping evaluation due to error on id={rid}: {e}\")\n",
        "            break\n",
        "\n",
        "    if not evaluated_ids:\n",
        "        print(\"\\nNo samples were successfully evaluated; nothing to save.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFinished with {len(evaluated_ids)} successfully evaluated unique IDs.\")\n",
        "\n",
        "    # Aggregate averages\n",
        "    summary = {\n",
        "        \"provider\": \"groq\",\n",
        "        \"model\": GROQ_MODEL_NAME,\n",
        "        \"num_samples_requested\": len(target_ids),\n",
        "        \"num_samples_evaluated\": len(evaluated_ids),\n",
        "        \"evaluated_ids\": sorted(evaluated_ids),\n",
        "        \"per_sample\": per_sample,\n",
        "        \"avg\": {},\n",
        "    }\n",
        "\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        summary[\"avg\"][which] = {}\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            scores = all_scores[which][dim]\n",
        "            if scores:\n",
        "                mean_val = float(statistics.mean(scores))\n",
        "                std_val = float(statistics.pstdev(scores)) if len(scores) > 1 else 0.0\n",
        "            else:\n",
        "                mean_val = std_val = 0.0\n",
        "            summary[\"avg\"][which][dim] = {\n",
        "                \"mean\": mean_val,\n",
        "                \"std\": std_val,\n",
        "                \"scores\": scores,\n",
        "            }\n",
        "\n",
        "    with open(LLM_JUDGE_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved Groq LLM-as-a-Judge scores to: {LLM_JUDGE_OUT}\")\n",
        "    print(\"Average scores (mean over evaluated samples):\")\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        print(f\"\\n{which.upper()}:\")\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            stats_dim = summary[\"avg\"][which][dim]\n",
        "            print(\n",
        "                f\"  {dim}: mean={stats_dim['mean']:.2f}, \"\n",
        "                f\"std={stats_dim['std']:.2f}, scores={stats_dim['scores']}\"\n",
        "            )\n",
        "\n",
        "# To run once your GROQ_API_KEY is set:\n",
        "# run_groq_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target IDs for evaluation (up to 10): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "No existing llm_judge_scores_groq.json found; starting fresh.\n",
            "Remaining IDs to evaluate: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "\n",
            "=== Evaluating id=0 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=1 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=2 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=3 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=4 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=5 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=6 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=7 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=8 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "=== Evaluating id=9 ===\n",
            "Calling Groq judge for BASE summary...\n",
            "Calling Groq judge for LoRA summary...\n",
            "\n",
            "Finished with 10 successfully evaluated unique IDs.\n",
            "\n",
            "Saved Groq LLM-as-a-Judge scores to: ./outputs/llm_judge_scores_groq.json\n",
            "Average scores (mean over evaluated samples):\n",
            "\n",
            "BASE:\n",
            "  fluency: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  factuality: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  coverage: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "LORA:\n",
            "  fluency: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  factuality: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  coverage: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "run_groq_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updated sys prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Part 4.B  LLM-as-a-Judge using Groq (llama-3.3-70b-versatile)\n",
        "# - Evaluates up to NUM_SAMPLES samples (base vs LoRA)\n",
        "# - Saves ./outputs/llm_judge_scores_groq.json\n",
        "# - Resumable: reuses previous results and only judges missing ids\n",
        "\n",
        "%pip install -q groq\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "from groq import Groq\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "COMPARISONS_PATH = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "LLM_JUDGE_OUT = os.path.join(OUTPUT_DIR, \"llm_judge_scores_groq.json\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"YOUR_GROQ_API_KEY_HERE\")  # set env var or replace string\n",
        "GROQ_MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "NUM_SAMPLES = 10\n",
        "MAX_INPUT_CHARS = 4000\n",
        "random.seed(42)\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# ----------------------\n",
        "# System + user prompt (updated, exact per assignment)\n",
        "# ----------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a strict evaluator. Rate the generated summary along three dimensions from 1 (poor) \"\n",
        "    \"to 5 (excellent). Provide only a JSON object as the final output.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Input: <<INPUT_TEXT>>\n",
        "\n",
        "Ground-Truth Abstract: <<GROUND_TRUTH>>\n",
        "\n",
        "Generated Summary: <<GENERATED_SUMMARY>>\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- For each dimension (fluency, factuality, coverage) return:\n",
        "\n",
        "  - \"score\": integer 1-5\n",
        "\n",
        "  - \"justification\": a short justification (max 40 words)\n",
        "\n",
        "  - \"errors\": a short list (max 3 items) describing factual errors or hallucinations; if none, return [].\n",
        "\n",
        "- Return a single valid JSON object with keys: \"fluency\", \"factuality\", \"coverage\".\n",
        "\n",
        "- Example output:\n",
        "\n",
        "{\n",
        "  \"fluency\": {\"score\": 4, \"justification\": \"Readable and grammatical\", \"errors\": []},\n",
        "  \"factuality\": {\"score\": 3, \"justification\": \"Missed experimental detail\", \"errors\": [\"Says dataset size is 1.5k rather than 15k\"]},\n",
        "  \"coverage\": {\"score\": 2, \"justification\": \"Omits method description\", \"errors\": [\"No mention of optimization method\"]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def truncate_text(text: str, max_chars: int = MAX_INPUT_CHARS) -> str:\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars] + \"\\n\\n(TRUNCATED)\"\n",
        "\n",
        "def build_user_prompt(input_text: str, ground_truth: str, generated_summary: str) -> str:\n",
        "    return (\n",
        "        USER_PROMPT_TEMPLATE\n",
        "        .replace(\"<<INPUT_TEXT>>\", truncate_text(input_text))\n",
        "        .replace(\"<<GROUND_TRUTH>>\", truncate_text(ground_truth))\n",
        "        .replace(\"<<GENERATED_SUMMARY>>\", truncate_text(generated_summary))\n",
        "    )\n",
        "\n",
        "# ----------------------\n",
        "# Single Groq call for one judgment\n",
        "# ----------------------\n",
        "def call_groq_judge(input_text: str,\n",
        "                    ground_truth: str,\n",
        "                    generated_summary: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calls Groq (llama-3.3-70b-versatile) with the SYSTEM/USER prompt.\n",
        "    Returns parsed JSON dict with keys: fluency, factuality, coverage.\n",
        "    On bad JSON or empty content, returns a neutral fallback.\n",
        "    \"\"\"\n",
        "    user_prompt = build_user_prompt(input_text, ground_truth, generated_summary)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=GROQ_MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_completion_tokens=512,\n",
        "        top_p=1.0,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    def fallback_judgment(reason: str = \"no_output\"):\n",
        "        return {\n",
        "            \"fluency\":   {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"factuality\":{\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"coverage\":  {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "        }\n",
        "\n",
        "    if not completion.choices:\n",
        "        return fallback_judgment(\"no_choices\")\n",
        "\n",
        "    content = (completion.choices[0].message.content or \"\").strip()\n",
        "    if not content:\n",
        "        return fallback_judgment(\"empty_content\")\n",
        "\n",
        "    # Extract JSON between first { and last }\n",
        "    try:\n",
        "        start = content.index(\"{\")\n",
        "        end = content.rindex(\"}\") + 1\n",
        "        json_str = content[start:end]\n",
        "    except ValueError:\n",
        "        json_str = content\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str_fixed = json_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        try:\n",
        "            parsed = json.loads(json_str_fixed)\n",
        "        except json.JSONDecodeError:\n",
        "            return fallback_judgment(\"json_parse_error\")\n",
        "\n",
        "    return parsed\n",
        "\n",
        "# ----------------------\n",
        "# Resumable evaluation over up to NUM_SAMPLES samples\n",
        "# ----------------------\n",
        "def run_groq_llm_judge_evaluation():\n",
        "    \"\"\"\n",
        "    Runs LLM-as-a-Judge over up to NUM_SAMPLES examples.\n",
        "    - If llm_judge_scores_groq.json exists, reuses previous results.\n",
        "    - Only calls Groq for remaining ids.\n",
        "    - On API error, stops and still saves accumulated results.\n",
        "    \"\"\"\n",
        "    # Load comparisons.jsonl\n",
        "    records = []\n",
        "    with open(COMPARISONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            if \"id\" not in rec:\n",
        "                rec[\"id\"] = idx\n",
        "            records.append(rec)\n",
        "\n",
        "    if not records:\n",
        "        raise RuntimeError(\"No records found in comparisons.jsonl\")\n",
        "\n",
        "    id_to_rec = {rec[\"id\"]: rec for rec in records}\n",
        "    all_ids = sorted(id_to_rec.keys())\n",
        "    target_ids = all_ids[:min(NUM_SAMPLES, len(all_ids))]\n",
        "\n",
        "    print(f\"Target IDs for evaluation (up to {NUM_SAMPLES}): {target_ids}\")\n",
        "\n",
        "    # Prepare containers\n",
        "    all_scores = {\n",
        "        \"base\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "        \"lora\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "    }\n",
        "    per_sample = []\n",
        "    evaluated_ids = set()\n",
        "\n",
        "    # Load existing results if any\n",
        "    if os.path.exists(LLM_JUDGE_OUT):\n",
        "        try:\n",
        "            with open(LLM_JUDGE_OUT, \"r\", encoding=\"utf-8\") as f:\n",
        "                existing = json.load(f)\n",
        "            for entry in existing.get(\"per_sample\", []):\n",
        "                rid = entry.get(\"id\")\n",
        "                if rid in target_ids:\n",
        "                    per_sample.append(entry)\n",
        "                    evaluated_ids.add(rid)\n",
        "                    for which in [\"base\", \"lora\"]:\n",
        "                        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                            score = int(entry[which][dim][\"score\"])\n",
        "                            all_scores[which][dim].append(score)\n",
        "            print(f\"Loaded existing judgments for IDs: {sorted(evaluated_ids)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Failed to load existing {LLM_JUDGE_OUT}: {e}\")\n",
        "    else:\n",
        "        print(\"No existing llm_judge_scores_groq.json found; starting fresh.\")\n",
        "\n",
        "    remaining_ids = [rid for rid in target_ids if rid not in evaluated_ids]\n",
        "    print(f\"Remaining IDs to evaluate: {remaining_ids}\")\n",
        "\n",
        "    # Evaluate remaining\n",
        "    for rid in remaining_ids:\n",
        "        rec = id_to_rec[rid]\n",
        "        input_text = rec[\"input_text\"]\n",
        "        ground_truth = rec[\"ground_truth\"]\n",
        "        base_summary = rec[\"base_summary\"]\n",
        "        lora_summary = rec[\"lora_summary\"]\n",
        "\n",
        "        print(f\"\\n=== Evaluating id={rid} ===\")\n",
        "        try:\n",
        "            print(\"Calling Groq judge for BASE summary...\")\n",
        "            base_judgment = call_groq_judge(input_text, ground_truth, base_summary)\n",
        "\n",
        "            print(\"Calling Groq judge for LoRA summary...\")\n",
        "            lora_judgment = call_groq_judge(input_text, ground_truth, lora_summary)\n",
        "\n",
        "            for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                b_score = int(base_judgment[dim][\"score\"])\n",
        "                l_score = int(lora_judgment[dim][\"score\"])\n",
        "                all_scores[\"base\"][dim].append(b_score)\n",
        "                all_scores[\"lora\"][dim].append(l_score)\n",
        "\n",
        "            per_sample.append(\n",
        "                {\n",
        "                    \"id\": rid,\n",
        "                    \"base\": base_judgment,\n",
        "                    \"lora\": lora_judgment,\n",
        "                }\n",
        "            )\n",
        "            evaluated_ids.add(rid)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Stopping evaluation due to error on id={rid}: {e}\")\n",
        "            break\n",
        "\n",
        "    if not evaluated_ids:\n",
        "        print(\"\\nNo samples were successfully evaluated; nothing to save.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFinished with {len(evaluated_ids)} successfully evaluated unique IDs.\")\n",
        "\n",
        "    # Aggregate averages\n",
        "    summary = {\n",
        "        \"provider\": \"groq\",\n",
        "        \"model\": GROQ_MODEL_NAME,\n",
        "        \"num_samples_requested\": len(target_ids),\n",
        "        \"num_samples_evaluated\": len(evaluated_ids),\n",
        "        \"evaluated_ids\": sorted(evaluated_ids),\n",
        "        \"per_sample\": per_sample,\n",
        "        \"avg\": {},\n",
        "    }\n",
        "\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        summary[\"avg\"][which] = {}\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            scores = all_scores[which][dim]\n",
        "            if scores:\n",
        "                mean_val = float(statistics.mean(scores))\n",
        "                std_val = float(statistics.pstdev(scores)) if len(scores) > 1 else 0.0\n",
        "            else:\n",
        "                mean_val = std_val = 0.0\n",
        "            summary[\"avg\"][which][dim] = {\n",
        "                \"mean\": mean_val,\n",
        "                \"std\": std_val,\n",
        "                \"scores\": scores,\n",
        "            }\n",
        "\n",
        "    with open(LLM_JUDGE_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved Groq LLM-as-a-Judge scores to: {LLM_JUDGE_OUT}\")\n",
        "    print(\"Average scores (mean over evaluated samples):\")\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        print(f\"\\n{which.upper()}:\")\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            stats_dim = summary[\"avg\"][which][dim]\n",
        "            print(\n",
        "                f\"  {dim}: mean={stats_dim['mean']:.2f}, \"\n",
        "                f\"std={stats_dim['std']:.2f}, scores={stats_dim['scores']}\"\n",
        "            )\n",
        "\n",
        "# To run once your GROQ_API_KEY is set:\n",
        "# run_groq_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target IDs for evaluation (up to 10): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Loaded existing judgments for IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Remaining IDs to evaluate: []\n",
            "\n",
            "Finished with 10 successfully evaluated unique IDs.\n",
            "\n",
            "Saved Groq LLM-as-a-Judge scores to: ./outputs/llm_judge_scores_groq.json\n",
            "Average scores (mean over evaluated samples):\n",
            "\n",
            "BASE:\n",
            "  fluency: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  factuality: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  coverage: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "LORA:\n",
            "  fluency: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  factuality: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  coverage: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "run_groq_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## meta-llama/llama-4-maverick-17b-128e-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Part 4.B  LLM-as-a-Judge using Groq (llama-3.3-70b-versatile)\n",
        "# - Evaluates up to NUM_SAMPLES samples (base vs LoRA)\n",
        "# - Saves ./outputs/llm_judge_scores_groq.json\n",
        "# - Resumable: reuses previous results and only judges missing ids\n",
        "\n",
        "%pip install -q groq\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "from groq import Groq\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "COMPARISONS_PATH = os.path.join(OUTPUT_DIR, \"comparisons.jsonl\")\n",
        "LLM_JUDGE_OUT = os.path.join(OUTPUT_DIR, \"llm_judge_scores_groq.json\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "GROQ_MODEL_NAME = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        "\n",
        "NUM_SAMPLES = 10\n",
        "MAX_INPUT_CHARS = 4000\n",
        "random.seed(42)\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# ----------------------\n",
        "# Exact LLM-as-a-Judge prompt (SYSTEM + USER) as specified\n",
        "# ----------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a strict evaluator. Rate the generated summary along three dimensions from 1 (poor) \"\n",
        "    \"to 5 (excellent). Provide only a JSON object as the final output.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Input: <<INPUT_TEXT>>\n",
        "\n",
        "Ground-Truth Abstract: <<GROUND_TRUTH>>\n",
        "\n",
        "Generated Summary: <<GENERATED_SUMMARY>>\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- For each dimension (fluency, factuality, coverage) return:\n",
        "\n",
        "  - \"score\": integer 1-5\n",
        "\n",
        "  - \"justification\": a short justification (max 40 words)\n",
        "\n",
        "  - \"errors\": a short list (max 3 items) describing factual errors or hallucinations; if none, return [].\n",
        "\n",
        "- Return a single valid JSON object with keys: \"fluency\", \"factuality\", \"coverage\".\n",
        "\n",
        "- Example output:\n",
        "\n",
        "{\n",
        "  \"fluency\": {\"score\": 4, \"justification\": \"Readable and grammatical\", \"errors\": []},\n",
        "  \"factuality\": {\"score\": 3, \"justification\": \"Missed experimental detail\", \"errors\": [\"Says dataset size is 1.5k rather than 15k\"]},\n",
        "  \"coverage\": {\"score\": 2, \"justification\": \"Omits method description\", \"errors\": [\"No mention of optimization method\"]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def truncate_text(text: str, max_chars: int = MAX_INPUT_CHARS) -> str:\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars] + \"\\n\\n(TRUNCATED)\"\n",
        "\n",
        "def build_user_prompt(input_text: str, ground_truth: str, generated_summary: str) -> str:\n",
        "    return (\n",
        "        USER_PROMPT_TEMPLATE\n",
        "        .replace(\"<<INPUT_TEXT>>\", truncate_text(input_text))\n",
        "        .replace(\"<<GROUND_TRUTH>>\", truncate_text(ground_truth))\n",
        "        .replace(\"<<GENERATED_SUMMARY>>\", truncate_text(generated_summary))\n",
        "    )\n",
        "\n",
        "# ----------------------\n",
        "# Single Groq call for one judgment\n",
        "# ----------------------\n",
        "def call_groq_judge(input_text: str,\n",
        "                    ground_truth: str,\n",
        "                    generated_summary: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calls Groq (llama-3.3-70b-versatile) with the SYSTEM/USER prompt.\n",
        "    Returns parsed JSON dict with keys: fluency, factuality, coverage.\n",
        "    On bad JSON or empty content, returns a neutral fallback.\n",
        "    \"\"\"\n",
        "    user_prompt = build_user_prompt(input_text, ground_truth, generated_summary)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=GROQ_MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_completion_tokens=512,\n",
        "        top_p=1.0,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    def fallback_judgment(reason: str = \"no_output\"):\n",
        "        return {\n",
        "            \"fluency\":   {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"factuality\":{\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"coverage\":  {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "        }\n",
        "\n",
        "    if not completion.choices:\n",
        "        return fallback_judgment(\"no_choices\")\n",
        "\n",
        "    content = (completion.choices[0].message.content or \"\").strip()\n",
        "    if not content:\n",
        "        return fallback_judgment(\"empty_content\")\n",
        "\n",
        "    # Extract JSON between first { and last }\n",
        "    try:\n",
        "        start = content.index(\"{\")\n",
        "        end = content.rindex(\"}\") + 1\n",
        "        json_str = content[start:end]\n",
        "    except ValueError:\n",
        "        json_str = content\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str_fixed = json_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        try:\n",
        "            parsed = json.loads(json_str_fixed)\n",
        "        except json.JSONDecodeError:\n",
        "            return fallback_judgment(\"json_parse_error\")\n",
        "\n",
        "    return parsed\n",
        "\n",
        "# ----------------------\n",
        "# Resumable evaluation over up to NUM_SAMPLES samples\n",
        "# ----------------------\n",
        "def run_groq_llm_judge_evaluation():\n",
        "    \"\"\"\n",
        "    Runs LLM-as-a-Judge over up to NUM_SAMPLES examples.\n",
        "    - If llm_judge_scores_groq.json exists, reuses previous results.\n",
        "    - Only calls Groq for remaining ids.\n",
        "    - On API error, stops and still saves accumulated results.\n",
        "    \"\"\"\n",
        "    # Load comparisons.jsonl\n",
        "    records = []\n",
        "    with open(COMPARISONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            if \"id\" not in rec:\n",
        "                rec[\"id\"] = idx\n",
        "            records.append(rec)\n",
        "\n",
        "    if not records:\n",
        "        raise RuntimeError(\"No records found in comparisons.jsonl\")\n",
        "\n",
        "    id_to_rec = {rec[\"id\"]: rec for rec in records}\n",
        "    all_ids = sorted(id_to_rec.keys())\n",
        "    target_ids = all_ids[:min(NUM_SAMPLES, len(all_ids))]\n",
        "\n",
        "    print(f\"Target IDs for evaluation (up to {NUM_SAMPLES}): {target_ids}\")\n",
        "\n",
        "    # Prepare containers\n",
        "    all_scores = {\n",
        "        \"base\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "        \"lora\": {\"fluency\": [], \"factuality\": [], \"coverage\": []},\n",
        "    }\n",
        "    per_sample = []\n",
        "    evaluated_ids = set()\n",
        "\n",
        "    # Load existing results if any\n",
        "    if os.path.exists(LLM_JUDGE_OUT):\n",
        "        try:\n",
        "            with open(LLM_JUDGE_OUT, \"r\", encoding=\"utf-8\") as f:\n",
        "                existing = json.load(f)\n",
        "            for entry in existing.get(\"per_sample\", []):\n",
        "                rid = entry.get(\"id\")\n",
        "                if rid in target_ids:\n",
        "                    per_sample.append(entry)\n",
        "                    evaluated_ids.add(rid)\n",
        "                    for which in [\"base\", \"lora\"]:\n",
        "                        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                            score = int(entry[which][dim][\"score\"])\n",
        "                            all_scores[which][dim].append(score)\n",
        "            print(f\"Loaded existing judgments for IDs: {sorted(evaluated_ids)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Failed to load existing {LLM_JUDGE_OUT}: {e}\")\n",
        "    else:\n",
        "        print(\"No existing llm_judge_scores_groq.json found; starting fresh.\")\n",
        "\n",
        "    remaining_ids = [rid for rid in target_ids if rid not in evaluated_ids]\n",
        "    print(f\"Remaining IDs to evaluate: {remaining_ids}\")\n",
        "\n",
        "    # Evaluate remaining\n",
        "    for rid in remaining_ids:\n",
        "        rec = id_to_rec[rid]\n",
        "        input_text = rec[\"input_text\"]\n",
        "        ground_truth = rec[\"ground_truth\"]\n",
        "        base_summary = rec[\"base_summary\"]\n",
        "        lora_summary = rec[\"lora_summary\"]\n",
        "\n",
        "        print(f\"\\n=== Evaluating id={rid} ===\")\n",
        "        try:\n",
        "            print(\"Calling Groq judge for BASE summary...\")\n",
        "            base_judgment = call_groq_judge(input_text, ground_truth, base_summary)\n",
        "\n",
        "            print(\"Calling Groq judge for LoRA summary...\")\n",
        "            lora_judgment = call_groq_judge(input_text, ground_truth, lora_summary)\n",
        "\n",
        "            for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "                b_score = int(base_judgment[dim][\"score\"])\n",
        "                l_score = int(lora_judgment[dim][\"score\"])\n",
        "                all_scores[\"base\"][dim].append(b_score)\n",
        "                all_scores[\"lora\"][dim].append(l_score)\n",
        "\n",
        "            per_sample.append(\n",
        "                {\n",
        "                    \"id\": rid,\n",
        "                    \"base\": base_judgment,\n",
        "                    \"lora\": lora_judgment,\n",
        "                }\n",
        "            )\n",
        "            evaluated_ids.add(rid)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Stopping evaluation due to error on id={rid}: {e}\")\n",
        "            break\n",
        "\n",
        "    if not evaluated_ids:\n",
        "        print(\"\\nNo samples were successfully evaluated; nothing to save.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFinished with {len(evaluated_ids)} successfully evaluated unique IDs.\")\n",
        "\n",
        "    # Aggregate averages\n",
        "    summary = {\n",
        "        \"provider\": \"groq\",\n",
        "        \"model\": GROQ_MODEL_NAME,\n",
        "        \"num_samples_requested\": len(target_ids),\n",
        "        \"num_samples_evaluated\": len(evaluated_ids),\n",
        "        \"evaluated_ids\": sorted(evaluated_ids),\n",
        "        \"per_sample\": per_sample,\n",
        "        \"avg\": {},\n",
        "    }\n",
        "\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        summary[\"avg\"][which] = {}\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            scores = all_scores[which][dim]\n",
        "            if scores:\n",
        "                mean_val = float(statistics.mean(scores))\n",
        "                std_val = float(statistics.pstdev(scores)) if len(scores) > 1 else 0.0\n",
        "            else:\n",
        "                mean_val = std_val = 0.0\n",
        "            summary[\"avg\"][which][dim] = {\n",
        "                \"mean\": mean_val,\n",
        "                \"std\": std_val,\n",
        "                \"scores\": scores,\n",
        "            }\n",
        "\n",
        "    with open(LLM_JUDGE_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved Groq LLM-as-a-Judge scores to: {LLM_JUDGE_OUT}\")\n",
        "    print(\"Average scores (mean over evaluated samples):\")\n",
        "    for which in [\"base\", \"lora\"]:\n",
        "        print(f\"\\n{which.upper()}:\")\n",
        "        for dim in [\"fluency\", \"factuality\", \"coverage\"]:\n",
        "            stats_dim = summary[\"avg\"][which][dim]\n",
        "            print(\n",
        "                f\"  {dim}: mean={stats_dim['mean']:.2f}, \"\n",
        "                f\"std={stats_dim['std']:.2f}, scores={stats_dim['scores']}\"\n",
        "            )\n",
        "\n",
        "# To run once your GROQ_API_KEY is set:\n",
        "# run_groq_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target IDs for evaluation (up to 10): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Loaded existing judgments for IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Remaining IDs to evaluate: []\n",
            "\n",
            "Finished with 10 successfully evaluated unique IDs.\n",
            "\n",
            "Saved Groq LLM-as-a-Judge scores to: ./outputs/llm_judge_scores_groq.json\n",
            "Average scores (mean over evaluated samples):\n",
            "\n",
            "BASE:\n",
            "  fluency: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  factuality: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  coverage: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "LORA:\n",
            "  fluency: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  factuality: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  coverage: mean=1.00, std=0.00, scores=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "run_groq_llm_judge_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* Running on public URL: https://48b88db84e404bbd6c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://48b88db84e404bbd6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a269976199884f88985a5e7b9fe57176",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "/workspace/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2532: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Part 5  Minimal local Gradio app for summarization + Groq LLM-as-a-Judge\n",
        "\n",
        "%pip install -q gradio pdfplumber transformers peft torch groq\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pdfplumber\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from groq import Groq\n",
        "\n",
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "BASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"   # base HF model name\n",
        "LORA_ADAPTER_DIR = \"./lora_summarizer_llama3/lora_adapter\"  # LoRA adapter from training\n",
        "\n",
        "MAX_INPUT_LENGTH = 2048\n",
        "MAX_NEW_TOKENS = 256\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"YOUR_GROQ_API_KEY_HERE\")  # set env var or replace\n",
        "GROQ_MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ----------------------\n",
        "# Tokenizer (shared)\n",
        "# ----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# ----------------------\n",
        "# Groq judge client + prompt\n",
        "# ----------------------\n",
        "client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY and \"YOUR_GROQ_API_KEY\" not in GROQ_API_KEY else None\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a strict evaluator. Rate the generated summary along three dimensions from 1 (poor) \"\n",
        "    \"to 5 (excellent). Provide only a JSON object as the final output.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Input: <<INPUT_TEXT>>\n",
        "\n",
        "Ground-Truth Abstract: <<GROUND_TRUTH>>\n",
        "\n",
        "Generated Summary: <<GENERATED_SUMMARY>>\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- For each dimension (fluency, factuality, coverage) return:\n",
        "\n",
        "  - \"score\": integer 1-5\n",
        "\n",
        "  - \"justification\": a short justification (max 40 words)\n",
        "\n",
        "  - \"errors\": a short list (max 3 items) describing factual errors or hallucinations; if none, return [].\n",
        "\n",
        "- Return a single valid JSON object with keys: \"fluency\", \"factuality\", \"coverage\".\n",
        "\n",
        "- Example output:\n",
        "\n",
        "{\n",
        "  \"fluency\": {\"score\": 4, \"justification\": \"Readable and grammatical\", \"errors\": []},\n",
        "  \"factuality\": {\"score\": 3, \"justification\": \"Missed experimental detail\", \"errors\": [\"Says dataset size is 1.5k rather than 15k\"]},\n",
        "  \"coverage\": {\"score\": 2, \"justification\": \"Omits method description\", \"errors\": [\"No mention of optimization method\"]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def truncate_text(text: str, max_chars: int = 4000) -> str:\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars] + \"\\n\\n(TRUNCATED)\"\n",
        "\n",
        "def build_user_prompt(input_text: str, ground_truth: str, generated_summary: str) -> str:\n",
        "    return (\n",
        "        USER_PROMPT_TEMPLATE\n",
        "        .replace(\"<<INPUT_TEXT>>\", truncate_text(input_text))\n",
        "        .replace(\"<<GROUND_TRUTH>>\", truncate_text(ground_truth))\n",
        "        .replace(\"<<GENERATED_SUMMARY>>\", truncate_text(generated_summary))\n",
        "    )\n",
        "\n",
        "def call_groq_judge(input_text: str, generated_summary: str) -> dict:\n",
        "    \"\"\"Call Groq judge; ground_truth not available in app -> pass empty string.\"\"\"\n",
        "    if client is None:\n",
        "        return {\n",
        "            \"fluency\":   {\"score\": None, \"justification\": \"No GROQ_API_KEY set\", \"errors\": []},\n",
        "            \"factuality\":{\"score\": None, \"justification\": \"No GROQ_API_KEY set\", \"errors\": []},\n",
        "            \"coverage\":  {\"score\": None, \"justification\": \"No GROQ_API_KEY set\", \"errors\": []},\n",
        "        }\n",
        "\n",
        "    ground_truth = \"\"  # not provided in interactive app\n",
        "    user_prompt = build_user_prompt(input_text, ground_truth, generated_summary)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=GROQ_MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_completion_tokens=512,\n",
        "        top_p=1.0,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    def fallback(reason: str):\n",
        "        return {\n",
        "            \"fluency\":   {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"factuality\":{\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "            \"coverage\":  {\"score\": 3, \"justification\": f\"Fallback: {reason}\", \"errors\": []},\n",
        "        }\n",
        "\n",
        "    if not completion.choices:\n",
        "        return fallback(\"no_choices\")\n",
        "\n",
        "    content = (completion.choices[0].message.content or \"\").strip()\n",
        "    if not content:\n",
        "        return fallback(\"empty_content\")\n",
        "\n",
        "    try:\n",
        "        start = content.index(\"{\")\n",
        "        end = content.rindex(\"}\") + 1\n",
        "        json_str = content[start:end]\n",
        "    except ValueError:\n",
        "        json_str = content\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            parsed = json.loads(json_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))\n",
        "        except json.JSONDecodeError:\n",
        "            return fallback(\"json_parse_error\")\n",
        "\n",
        "    return parsed\n",
        "\n",
        "# ----------------------\n",
        "# Model loading & summarization helpers\n",
        "# ----------------------\n",
        "def load_base_model():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_NAME,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_lora_model():\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_NAME,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base, LORA_ADAPTER_DIR)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def generate_summary(model, text: str) -> str:\n",
        "    prompt = (\n",
        "        \"Summarize the following scientific article.\\n\\n\"\n",
        "        \"Article:\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "    )\n",
        "    # IMPORTANT: do NOT move to CUDA manually when using device_map / accelerate\n",
        "    # accelerate will route tensors to the right device(s) internally.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    summary = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "    return summary\n",
        "def read_uploaded_file(text: str, file) -> str:\n",
        "    if file is not None:\n",
        "        path = file.name\n",
        "        if path.lower().endswith(\".txt\"):\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                return f.read()\n",
        "        elif path.lower().endswith(\".pdf\"):\n",
        "            full_text = []\n",
        "            with pdfplumber.open(path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    full_text.append(page.extract_text() or \"\")\n",
        "            return \"\\n\\n\".join(full_text)\n",
        "        else:\n",
        "            return f\"Unsupported file type: {os.path.basename(path)}\"\n",
        "    return text or \"\"\n",
        "\n",
        "# ----------------------\n",
        "# Gradio callback functions\n",
        "# ----------------------\n",
        "def summarize_base(text, file):\n",
        "    article = read_uploaded_file(text, file).strip()\n",
        "    if not article:\n",
        "        return \"No input text provided.\", \"\", \"\", \"No summary generated.\"\n",
        "\n",
        "    try:\n",
        "        model = load_base_model()\n",
        "        summary = generate_summary(model, article)\n",
        "    finally:\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    judge = call_groq_judge(article, summary)\n",
        "    judge_str = json.dumps(judge, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return truncate_text(article), summary, \"(not generated yet)\", judge_str\n",
        "\n",
        "def summarize_lora(text, file):\n",
        "    article = read_uploaded_file(text, file).strip()\n",
        "    if not article:\n",
        "        return \"No input text provided.\", \"\", \"\", \"No summary generated.\"\n",
        "\n",
        "    try:\n",
        "        model = load_lora_model()\n",
        "        summary = generate_summary(model, article)\n",
        "    finally:\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    judge = call_groq_judge(article, summary)\n",
        "    judge_str = json.dumps(judge, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return truncate_text(article), \"(not generated yet)\", summary, judge_str\n",
        "\n",
        "# ----------------------\n",
        "# Build Gradio UI\n",
        "# ----------------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Smart Summarizer Demo (Base vs LoRA + Groq LLM-as-a-Judge)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_text = gr.Textbox(\n",
        "                label=\"Paste paper text\",\n",
        "                lines=15,\n",
        "                placeholder=\"Paste the article text here...\",\n",
        "            )\n",
        "            input_file = gr.File(label=\"Or upload .txt / .pdf\", file_types=[\".txt\", \".pdf\"])\n",
        "\n",
        "            with gr.Row():\n",
        "                btn_base = gr.Button(\"Summarize (Base)\", variant=\"secondary\")\n",
        "                btn_lora = gr.Button(\"Summarize (LoRA)\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            original_out = gr.Textbox(label=\"Original Text (truncated)\", lines=12)\n",
        "            base_out = gr.Textbox(label=\"Base Summary\", lines=10)\n",
        "            lora_out = gr.Textbox(label=\"LoRA Summary\", lines=10)\n",
        "            judge_out = gr.Textbox(label=\"LLM-as-a-Judge Scores (Groq)\", lines=10)\n",
        "\n",
        "    btn_base.click(\n",
        "        summarize_base,\n",
        "        inputs=[input_text, input_file],\n",
        "        outputs=[original_out, base_out, lora_out, judge_out],\n",
        "    )\n",
        "    btn_lora.click(\n",
        "        summarize_lora,\n",
        "        inputs=[input_text, input_file],\n",
        "        outputs=[original_out, base_out, lora_out, judge_out],\n",
        "    )\n",
        "\n",
        "# For a public link, set share=True. For local-only, use share=False.\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16G\tlora_summarizer_llama3\n",
            "15G\tlora_summarizer_llama3/full_model\n",
            "30M\tlora_summarizer_llama3/lora_adapter\n"
          ]
        }
      ],
      "source": [
        "# Total size of fine-tuned artifacts\n",
        "!du -sh lora_summarizer_llama3\n",
        "\n",
        "# Size of full saved model\n",
        "!du -sh lora_summarizer_llama3/full_model\n",
        "\n",
        "# Size of LoRA adapter only\n",
        "!du -sh lora_summarizer_llama3/lora_adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo id: Mujtaba007/llama3-arxiv-lora\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "HfFolder.save_token(HF_TOKEN)\n",
        "api = HfApi()\n",
        "\n",
        "HF_USERNAME = \"Mujtaba007\"\n",
        "REPO_ID = f\"{HF_USERNAME}/llama3-arxiv-lora\"\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=REPO_ID,     # <- use repo_id, not name\n",
        "    private=True,        # False if you want it public\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "print(\"Repo id:\", REPO_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7a2de6e688342c5afe03f9f3486d815",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c548abc0ff61466590a741c92e58aea5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Mujtaba007/llama3-arxiv-lora/commit/d2031eb4adac50db094fb00d6c45aa7d36292f8f', commit_message='Add LoRA adapter', commit_description='', oid='d2031eb4adac50db094fb00d6c45aa7d36292f8f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mujtaba007/llama3-arxiv-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='Mujtaba007/llama3-arxiv-lora'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=REPO_ID,\n",
        "    folder_path=\"./lora_summarizer_llama3/lora_adapter\",\n",
        "    path_in_repo=\"\",\n",
        "    token=HF_TOKEN,\n",
        "    commit_message=\"Add LoRA adapter\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf5090",
      "language": "python",
      "name": "tf5090"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
